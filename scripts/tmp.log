Starting generator...
Starting monoceros...
Starting Prometheus...
2025/07/23 08:11:58 Metrics generator listening on :9100/metrics
2025/07/23 08:11:58 {r1_node_1 r1 r1_node_1:5001 r1_node_1:7001 r1_node_1:6001 r1_node_1:8001 r1_node_1 r1_node_1:7001 r1_node_1 r1_node_1:6001 {5} /var/log/monoceros}
GLOBAL HV2025/07/23 08:11:58 hyparview.go:57: HyParView node r1_node_1 initialized at r1_node_1:7001
GLOBAL HV2025/07/23 08:11:58 hyparview.go:166: try lock
REGION HV2025/07/23 08:11:58 hyparview.go:57: HyParView node r1_node_1 initialized at r1_node_1:6001
REGION HV2025/07/23 08:11:58 hyparview.go:156: try lock
REGION PT2025/07/23 08:11:58 plumtree.go:302: r1_node_1 - Message subscription started
REGION HV2025/07/23 08:11:58 hyparview.go:166: try lock
REGION PT2025/07/23 08:11:58 plumtree.go:68: r1_node_1 - Plumtree initialized peers []
RRN HV2025/07/23 08:11:58 hyparview.go:57: HyParView node r1_node_1 initialized at r1_node_1:8001
RRN HV2025/07/23 08:11:58 hyparview.go:156: try lock
RRN PT2025/07/23 08:11:58 plumtree.go:302: r1_node_1 - Message subscription started
RRN HV2025/07/23 08:11:58 hyparview.go:166: try lock
RRN PT2025/07/23 08:11:58 plumtree.go:68: r1_node_1 - Plumtree initialized peers []
GLOBAL HV2025/07/23 08:11:58 hyparview.go:74: r1_node_1 attempting to join via r1_node_1 (r1_node_1:7001)
GLOBAL HV2025/07/23 08:11:58 hyparview.go:76: try lock
GLOBAL HV2025/07/23 08:11:58 conn_tcp.go:149: Server listening on 0.0.0.0:7001
REGION HV2025/07/23 08:11:58 hyparview.go:74: r1_node_1 attempting to join via r1_node_1 (r1_node_1:6001)
REGION HV2025/07/23 08:11:58 hyparview.go:76: try lock
REGION HV2025/07/23 08:11:58 conn_tcp.go:149: Server listening on 0.0.0.0:6001
GLOBAL HV2025/07/23 08:11:58 hyparview.go:156: try lock
Waiting for quit signal...
MONOCEROS2025/07/23 08:11:58 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:58 monoceros.go:256: try promote
REGION PT2025/07/23 08:11:58 plumtree.go:216: try lock
REGION PT2025/07/23 08:11:58 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:11:58 plumtree.go:220: r1_node_1 - []
MONOCEROS2025/07/23 08:11:58 monoceros.go:266: peers num 0 now time 1753258318 expected aggregation time 5.2
MONOCEROS2025/07/23 08:11:58 monoceros.go:276: promoting RN
REGION PT2025/07/23 08:11:58 plumtree.go:95: try lock
REGION PT2025/07/23 08:11:58 plumtree.go:105: r1_node_1 - tree created RN_r1_node_1
MONOCEROS2025/07/23 08:11:58 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:58 monoceros.go:284: try lock
MONOCEROS2025/07/23 08:11:58 monoceros.go:788: try lock
MONOCEROS2025/07/23 08:11:58 monoceros.go:791: tree constructed in RN, should join RRN {RN_r1_node_1 r1_node_1}
ts=2025-07-23T08:11:58.549Z caller=main.go:589 level=info msg="No time or size retention was set so using the default time retention" duration=15d
ts=2025-07-23T08:11:58.549Z caller=main.go:633 level=info msg="Starting Prometheus Server" mode=server version="(version=2.53.3, branch=HEAD, revision=1491d29fb1e8f8acbab29fd54fd4ce9be2cbd7bc)"
ts=2025-07-23T08:11:58.549Z caller=main.go:638 level=info build_context="(go=go1.22.8, platform=linux/amd64, user=root@c6939e39a10c, date=20241105-12:18:07, tags=netgo,builtinassets,stringlabels)"
ts=2025-07-23T08:11:58.549Z caller=main.go:639 level=info host_details="(Linux 6.5.11-linuxkit #1 SMP PREEMPT_DYNAMIC Mon Dec  4 10:03:25 UTC 2023 x86_64 r1_node_1 (none))"
ts=2025-07-23T08:11:58.549Z caller=main.go:640 level=info fd_limits="(soft=1048576, hard=1048576)"
ts=2025-07-23T08:11:58.549Z caller=main.go:641 level=info vm_limits="(soft=unlimited, hard=unlimited)"
ts=2025-07-23T08:11:58.553Z caller=web.go:568 level=info component=web msg="Start listening for connections" address=0.0.0.0:9090
ts=2025-07-23T08:11:58.555Z caller=main.go:1148 level=info msg="Starting TSDB ..."
ts=2025-07-23T08:11:58.556Z caller=tls_config.go:313 level=info component=web msg="Listening on" address=[::]:9090
ts=2025-07-23T08:11:58.557Z caller=tls_config.go:316 level=info component=web msg="TLS is disabled." http2=false address=[::]:9090
ts=2025-07-23T08:11:58.562Z caller=head.go:626 level=info component=tsdb msg="Replaying on-disk memory mappable chunks if any"
ts=2025-07-23T08:11:58.562Z caller=head.go:713 level=info component=tsdb msg="On-disk memory mappable chunks replay completed" duration=23.865µs
ts=2025-07-23T08:11:58.562Z caller=head.go:721 level=info component=tsdb msg="Replaying WAL, this may take a while"
ts=2025-07-23T08:11:58.563Z caller=head.go:793 level=info component=tsdb msg="WAL segment loaded" segment=0 maxSegment=0
ts=2025-07-23T08:11:58.563Z caller=head.go:830 level=info component=tsdb msg="WAL replay completed" checkpoint_replay_duration=43.317µs wal_replay_duration=981.48µs wbl_replay_duration=115ns chunk_snapshot_load_duration=0s mmap_chunk_replay_duration=23.865µs total_replay_duration=1.108145ms
ts=2025-07-23T08:11:58.566Z caller=main.go:1169 level=info fs_type=EXT4_SUPER_MAGIC
ts=2025-07-23T08:11:58.566Z caller=main.go:1172 level=info msg="TSDB started"
ts=2025-07-23T08:11:58.566Z caller=main.go:1354 level=info msg="Loading configuration file" filename=/etc/prometheus/prometheus.yml
ts=2025-07-23T08:11:58.567Z caller=main.go:1391 level=info msg="updated GOGC" old=100 new=75
ts=2025-07-23T08:11:58.567Z caller=main.go:1402 level=info msg="Completed loading of configuration file" filename=/etc/prometheus/prometheus.yml totalDuration=1.167827ms db_storage=15.799µs remote_storage=7.899µs web_handler=286ns query_engine=7.332µs scrape=505.782µs scrape_sd=16.67µs notify=700ns notify_sd=580ns rules=16.266µs tracing=49.503µs
ts=2025-07-23T08:11:58.567Z caller=main.go:1133 level=info msg="Server is ready to receive web requests."
ts=2025-07-23T08:11:58.567Z caller=manager.go:164 level=info component="rule manager" msg="Starting rule manager..."
MONOCEROS2025/07/23 08:11:58 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:58 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:58 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:58 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:58 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:58 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:58 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:58 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:59 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:59 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:59 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:59 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:59 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:59 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:59 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:59 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:59 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:59 monoceros.go:250: try lock
GLOBAL HV2025/07/23 08:11:59 conn_tcp.go:159: new TCP connection 192.168.240.3:41898
GLOBAL HV2025/07/23 08:11:59 hyparview.go:214: try lock
GLOBAL HV2025/07/23 08:11:59 msg_handlers.go:12: try lock
GLOBAL HV2025/07/23 08:11:59 msg_handlers.go:23: received Join message self r1_node_1 from r1_node_2 listenAddress r1_node_2:7001
GLOBAL HV2025/07/23 08:11:59 msg_handlers.go:43: added peer to active view peerID r1_node_2 address r1_node_2:7001
MONOCEROS2025/07/23 08:11:59 monoceros.go:775: try lock
MONOCEROS2025/07/23 08:11:59 monoceros.go:778: global network peer up
MONOCEROS2025/07/23 08:11:59 monoceros.go:780: already synced, no need to send sync req
2025/07/23 08:11:59 nothing to send to peer
GLOBAL HV2025/07/23 08:11:59 hyparview.go:214: try lock
2025/07/23 08:11:59 received gossip msg [79 153 128 104 0 0 0 0 2]
2025/07/23 08:11:59 Received: [2]
MONOCEROS2025/07/23 08:11:59 monoceros.go:424: try lock
MONOCEROS2025/07/23 08:11:59 monoceros.go:427: received global network msg [2]
MONOCEROS2025/07/23 08:11:59 monoceros.go:456: received sync req
MONOCEROS2025/07/23 08:11:59 monoceros.go:468: sending sync resp [3 123 34 82 101 103 105 111 110 97 108 82 111 111 116 65 100 100 114 101 115 115 101 115 34 58 123 125 44 34 82 101 103 105 111 110 97 108 82 111 111 116 82 101 103 105 111 110 115 34 58 123 125 44 34 82 117 108 101 115 34 58 91 123 34 73 68 34 58 34 34 44 34 73 110 112 117 116 83 101 108 101 99 116 111 114 34 58 123 34 78 97 109 101 34 58 34 97 112 112 95 109 101 109 111 114 121 95 117 115 97 103 101 95 98 121 116 101 115 34 44 34 76 97 98 101 108 115 34 58 110 117 108 108 125 44 34 70 117 110 99 34 58 51 44 34 79 117 116 112 117 116 34 58 123 34 78 97 109 101 34 58 34 116 111 116 97 108 95 97 112 112 95 109 101 109 111 114 121 95 117 115 97 103 101 95 98 121 116 101 115 34 44 34 76 97 98 101 108 115 34 58 123 34 102 117 110 99 34 58 34 115 117 109 34 125 125 125 44 123 34 73 68 34 58 34 34 44 34 73 110 112 117 116 83 101 108 101 99 116 111 114 34 58 123 34 78 97 109 101 34 58 34 97 112 112 95 109 101 109 111 114 121 95 117 115 97 103 101 95 98 121 116 101 115 34 44 34 76 97 98 101 108 115 34 58 110 117 108 108 125 44 34 70 117 110 99 34 58 52 44 34 79 117 116 112 117 116 34 58 123 34 78 97 109 101 34 58 34 97 118 103 95 97 112 112 95 109 101 109 111 114 121 95 117 115 97 103 101 95 98 121 116 101 115 34 44 34 76 97 98 101 108 115 34 58 123 34 102 117 110 99 34 58 34 97 118 103 34 125 125 125 44 123 34 73 68 34 58 34 34 44 34 73 110 112 117 116 83 101 108 101 99 116 111 114 34 58 123 34 78 97 109 101 34 58 34 97 118 103 95 97 112 112 95 109 101 109 111 114 121 95 117 115 97 103 101 95 98 121 116 101 115 34 44 34 76 97 98 101 108 115 34 58 110 117 108 108 125 44 34 70 117 110 99 34 58 52 44 34 79 117 116 112 117 116 34 58 123 34 78 97 109 101 34 58 34 97 118 103 95 97 112 112 95 109 101 109 111 114 121 95 117 115 97 103 101 95 98 121 116 101 115 34 44 34 76 97 98 101 108 115 34 58 123 34 103 108 111 98 97 108 34 58 34 121 34 125 125 125 44 123 34 73 68 34 58 34 34 44 34 73 110 112 117 116 83 101 108 101 99 116 111 114 34 58 123 34 78 97 109 101 34 58 34 116 111 116 97 108 95 97 112 112 95 109 101 109 111 114 121 95 117 115 97 103 101 95 98 121 116 101 115 34 44 34 76 97 98 101 108 115 34 58 110 117 108 108 125 44 34 70 117 110 99 34 58 51 44 34 79 117 116 112 117 116 34 58 123 34 78 97 109 101 34 58 34 116 111 116 97 108 95 97 112 112 95 109 101 109 111 114 121 95 117 115 97 103 101 95 98 121 116 101 115 34 44 34 76 97 98 101 108 115 34 58 123 34 103 108 111 98 97 108 34 58 34 121 34 125 125 125 93 125]
2025/07/23 08:11:59 gn sending msg to peer
2025/07/23 08:11:59 quit broadcasting signal ...
REGION HV2025/07/23 08:11:59 conn_tcp.go:159: new TCP connection 192.168.240.3:43200
REGION HV2025/07/23 08:11:59 hyparview.go:214: try lock
REGION HV2025/07/23 08:11:59 msg_handlers.go:12: try lock
REGION HV2025/07/23 08:11:59 msg_handlers.go:23: received Join message self r1_node_1 from r1_node_2 listenAddress r1_node_2:6001
REGION HV2025/07/23 08:11:59 msg_handlers.go:43: added peer to active view peerID r1_node_2 address r1_node_2:6001
REGION PT2025/07/23 08:11:59 plumtree.go:325: r1_node_1 - Processing onPeerUp peer: r1_node_2
REGION PT2025/07/23 08:11:59 plumtree.go:326: try lock
REGION PT2025/07/23 08:11:59 plumtree.go:334: r1_node_1 - Added peer r1_node_2 to peers
REGION PT2025/07/23 08:11:59 tree.go:196: r1_node_1 - Processing onPeerUp peer: r1_node_2
REGION PT2025/07/23 08:11:59 tree.go:197: r1_node_1 - eager push peers [] lazy push peers []
REGION PT2025/07/23 08:11:59 tree.go:202: r1_node_1 - Added peer r1_node_2 to eager push peers
REGION PT2025/07/23 08:11:59 tree.go:204: r1_node_1 - eager push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700}] lazy push peers []
MONOCEROS2025/07/23 08:11:59 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:59 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:59 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:59 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:59 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:59 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:59 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:59 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:59 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:11:59 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:00 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:00 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:00 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:00 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:00 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:00 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:00 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:00 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:00 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:00 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:00 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:00 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:00 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:00 monoceros.go:250: try lock
GLOBAL HV2025/07/23 08:12:00 conn_tcp.go:159: new TCP connection 192.168.240.4:57246
GLOBAL HV2025/07/23 08:12:00 hyparview.go:214: try lock
GLOBAL HV2025/07/23 08:12:00 msg_handlers.go:12: try lock
GLOBAL HV2025/07/23 08:12:00 msg_handlers.go:23: received Join message self r1_node_1 from r1_node_3 listenAddress r1_node_3:7001
GLOBAL HV2025/07/23 08:12:00 msg_handlers.go:43: added peer to active view peerID r1_node_3 address r1_node_3:7001
MONOCEROS2025/07/23 08:12:00 monoceros.go:775: try lock
MONOCEROS2025/07/23 08:12:00 monoceros.go:778: global network peer up
MONOCEROS2025/07/23 08:12:00 monoceros.go:780: already synced, no need to send sync req
2025/07/23 08:12:00 nothing to send to peer
GLOBAL HV2025/07/23 08:12:00 hyparview.go:214: try lock
2025/07/23 08:12:00 received gossip msg [80 153 128 104 0 0 0 0 2]
2025/07/23 08:12:00 Received: [2]
MONOCEROS2025/07/23 08:12:00 monoceros.go:424: try lock
MONOCEROS2025/07/23 08:12:00 monoceros.go:427: received global network msg [2]
MONOCEROS2025/07/23 08:12:00 monoceros.go:456: received sync req
MONOCEROS2025/07/23 08:12:00 monoceros.go:468: sending sync resp [3 123 34 82 101 103 105 111 110 97 108 82 111 111 116 65 100 100 114 101 115 115 101 115 34 58 123 125 44 34 82 101 103 105 111 110 97 108 82 111 111 116 82 101 103 105 111 110 115 34 58 123 125 44 34 82 117 108 101 115 34 58 91 123 34 73 68 34 58 34 34 44 34 73 110 112 117 116 83 101 108 101 99 116 111 114 34 58 123 34 78 97 109 101 34 58 34 97 112 112 95 109 101 109 111 114 121 95 117 115 97 103 101 95 98 121 116 101 115 34 44 34 76 97 98 101 108 115 34 58 110 117 108 108 125 44 34 70 117 110 99 34 58 51 44 34 79 117 116 112 117 116 34 58 123 34 78 97 109 101 34 58 34 116 111 116 97 108 95 97 112 112 95 109 101 109 111 114 121 95 117 115 97 103 101 95 98 121 116 101 115 34 44 34 76 97 98 101 108 115 34 58 123 34 102 117 110 99 34 58 34 115 117 109 34 125 125 125 44 123 34 73 68 34 58 34 34 44 34 73 110 112 117 116 83 101 108 101 99 116 111 114 34 58 123 34 78 97 109 101 34 58 34 97 112 112 95 109 101 109 111 114 121 95 117 115 97 103 101 95 98 121 116 101 115 34 44 34 76 97 98 101 108 115 34 58 110 117 108 108 125 44 34 70 117 110 99 34 58 52 44 34 79 117 116 112 117 116 34 58 123 34 78 97 109 101 34 58 34 97 118 103 95 97 112 112 95 109 101 109 111 114 121 95 117 115 97 103 101 95 98 121 116 101 115 34 44 34 76 97 98 101 108 115 34 58 123 34 102 117 110 99 34 58 34 97 118 103 34 125 125 125 44 123 34 73 68 34 58 34 34 44 34 73 110 112 117 116 83 101 108 101 99 116 111 114 34 58 123 34 78 97 109 101 34 58 34 97 118 103 95 97 112 112 95 109 101 109 111 114 121 95 117 115 97 103 101 95 98 121 116 101 115 34 44 34 76 97 98 101 108 115 34 58 110 117 108 108 125 44 34 70 117 110 99 34 58 52 44 34 79 117 116 112 117 116 34 58 123 34 78 97 109 101 34 58 34 97 118 103 95 97 112 112 95 109 101 109 111 114 121 95 117 115 97 103 101 95 98 121 116 101 115 34 44 34 76 97 98 101 108 115 34 58 123 34 103 108 111 98 97 108 34 58 34 121 34 125 125 125 44 123 34 73 68 34 58 34 34 44 34 73 110 112 117 116 83 101 108 101 99 116 111 114 34 58 123 34 78 97 109 101 34 58 34 116 111 116 97 108 95 97 112 112 95 109 101 109 111 114 121 95 117 115 97 103 101 95 98 121 116 101 115 34 44 34 76 97 98 101 108 115 34 58 110 117 108 108 125 44 34 70 117 110 99 34 58 51 44 34 79 117 116 112 117 116 34 58 123 34 78 97 109 101 34 58 34 116 111 116 97 108 95 97 112 112 95 109 101 109 111 114 121 95 117 115 97 103 101 95 98 121 116 101 115 34 44 34 76 97 98 101 108 115 34 58 123 34 103 108 111 98 97 108 34 58 34 121 34 125 125 125 93 125]
2025/07/23 08:12:00 gn sending msg to peer
2025/07/23 08:12:00 quit broadcasting signal ...
REGION HV2025/07/23 08:12:00 hyparview.go:214: try lock
REGION HV2025/07/23 08:12:00 msg_handlers.go:89: try lock
REGION HV2025/07/23 08:12:00 msg_handlers.go:100: received ForwardJoin message self r1_node_1 from r1_node_3 TTL 3
REGION HV2025/07/23 08:12:00 msg_handlers.go:138: added peer to active view via forward join peerID r1_node_3 address r1_node_3:6001
REGION PT2025/07/23 08:12:00 plumtree.go:325: r1_node_1 - Processing onPeerUp peer: r1_node_3
REGION PT2025/07/23 08:12:00 plumtree.go:326: try lock
REGION PT2025/07/23 08:12:00 plumtree.go:334: r1_node_1 - Added peer r1_node_3 to peers
REGION PT2025/07/23 08:12:00 tree.go:196: r1_node_1 - Processing onPeerUp peer: r1_node_3
REGION PT2025/07/23 08:12:00 tree.go:197: r1_node_1 - eager push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700}] lazy push peers []
REGION PT2025/07/23 08:12:00 tree.go:202: r1_node_1 - Added peer r1_node_3 to eager push peers
REGION PT2025/07/23 08:12:00 tree.go:204: r1_node_1 - eager push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000}] lazy push peers []
MONOCEROS2025/07/23 08:12:00 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:00 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:00 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:00 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:00 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:00 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:01 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:01 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:01 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:01 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:01 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:01 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:01 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:01 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:01 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:01 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:01 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:01 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:01 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:01 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:01 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:01 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:01 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:01 monoceros.go:250: try lock
GLOBAL HV2025/07/23 08:12:01 hyparview.go:214: try lock
GLOBAL HV2025/07/23 08:12:01 msg_handlers.go:89: try lock
GLOBAL HV2025/07/23 08:12:01 msg_handlers.go:100: received ForwardJoin message self r1_node_1 from r1_node_4 TTL 3
GLOBAL HV2025/07/23 08:12:01 msg_handlers.go:143: added peer to passive view via forward join peerID r1_node_4
GLOBAL HV2025/07/23 08:12:01 hyparview.go:214: try lock
GLOBAL HV2025/07/23 08:12:01 msg_handlers.go:89: try lock
GLOBAL HV2025/07/23 08:12:01 msg_handlers.go:100: received ForwardJoin message self r1_node_1 from r1_node_4 TTL 2
GLOBAL HV2025/07/23 08:12:01 hyparview.go:214: try lock
GLOBAL HV2025/07/23 08:12:01 msg_handlers.go:89: try lock
GLOBAL HV2025/07/23 08:12:01 msg_handlers.go:100: received ForwardJoin message self r1_node_1 from r1_node_4 TTL 0
GLOBAL HV2025/07/23 08:12:01 msg_handlers.go:138: added peer to active view via forward join peerID r1_node_4 address r1_node_4:7001
MONOCEROS2025/07/23 08:12:01 monoceros.go:775: try lock
MONOCEROS2025/07/23 08:12:01 monoceros.go:778: global network peer up
MONOCEROS2025/07/23 08:12:01 monoceros.go:780: already synced, no need to send sync req
2025/07/23 08:12:01 nothing to send to peer
REGION HV2025/07/23 08:12:01 hyparview.go:214: try lock
REGION HV2025/07/23 08:12:01 msg_handlers.go:89: try lock
REGION HV2025/07/23 08:12:01 msg_handlers.go:100: received ForwardJoin message self r1_node_1 from r1_node_4 TTL 3
REGION HV2025/07/23 08:12:01 msg_handlers.go:143: added peer to passive view via forward join peerID r1_node_4
REGION HV2025/07/23 08:12:01 hyparview.go:214: try lock
REGION HV2025/07/23 08:12:01 msg_handlers.go:89: try lock
REGION HV2025/07/23 08:12:01 msg_handlers.go:100: received ForwardJoin message self r1_node_1 from r1_node_4 TTL 2
REGION HV2025/07/23 08:12:01 hyparview.go:214: try lock
REGION HV2025/07/23 08:12:01 msg_handlers.go:89: try lock
REGION HV2025/07/23 08:12:01 msg_handlers.go:100: received ForwardJoin message self r1_node_1 from r1_node_4 TTL 0
REGION HV2025/07/23 08:12:01 msg_handlers.go:138: added peer to active view via forward join peerID r1_node_4 address r1_node_4:6001
REGION PT2025/07/23 08:12:01 plumtree.go:325: r1_node_1 - Processing onPeerUp peer: r1_node_4
REGION PT2025/07/23 08:12:01 plumtree.go:326: try lock
REGION PT2025/07/23 08:12:01 plumtree.go:334: r1_node_1 - Added peer r1_node_4 to peers
REGION PT2025/07/23 08:12:01 tree.go:196: r1_node_1 - Processing onPeerUp peer: r1_node_4
REGION PT2025/07/23 08:12:01 tree.go:197: r1_node_1 - eager push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000}] lazy push peers []
REGION PT2025/07/23 08:12:01 tree.go:202: r1_node_1 - Added peer r1_node_4 to eager push peers
REGION PT2025/07/23 08:12:01 tree.go:204: r1_node_1 - eager push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340}] lazy push peers []
MONOCEROS2025/07/23 08:12:01 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:01 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:02 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:02 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:02 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:02 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:02 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:02 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:02 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:02 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:02 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:02 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:02 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:02 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:02 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:02 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:02 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:02 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:02 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:02 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:02 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:02 monoceros.go:250: try lock
GLOBAL HV2025/07/23 08:12:03 hyparview.go:214: try lock
GLOBAL HV2025/07/23 08:12:03 msg_handlers.go:89: try lock
GLOBAL HV2025/07/23 08:12:03 msg_handlers.go:100: received ForwardJoin message self r1_node_1 from r1_node_5 TTL 3
GLOBAL HV2025/07/23 08:12:03 msg_handlers.go:143: added peer to passive view via forward join peerID r1_node_5
REGION HV2025/07/23 08:12:03 conn_tcp.go:159: new TCP connection 192.168.240.6:58506
GLOBAL HV2025/07/23 08:12:03 hyparview.go:214: try lock
GLOBAL HV2025/07/23 08:12:03 msg_handlers.go:89: try lock
GLOBAL HV2025/07/23 08:12:03 msg_handlers.go:100: received ForwardJoin message self r1_node_1 from r1_node_5 TTL 2
GLOBAL HV2025/07/23 08:12:03 hyparview.go:214: try lock
GLOBAL HV2025/07/23 08:12:03 msg_handlers.go:89: try lock
GLOBAL HV2025/07/23 08:12:03 msg_handlers.go:100: received ForwardJoin message self r1_node_1 from r1_node_5 TTL 2
REGION HV2025/07/23 08:12:03 hyparview.go:214: try lock
REGION HV2025/07/23 08:12:03 msg_handlers.go:12: try lock
REGION HV2025/07/23 08:12:03 msg_handlers.go:23: received Join message self r1_node_1 from r1_node_5 listenAddress r1_node_5:6001
REGION HV2025/07/23 08:12:03 msg_handlers.go:43: added peer to active view peerID r1_node_5 address r1_node_5:6001
REGION PT2025/07/23 08:12:03 plumtree.go:325: r1_node_1 - Processing onPeerUp peer: r1_node_5
REGION PT2025/07/23 08:12:03 plumtree.go:326: try lock
REGION PT2025/07/23 08:12:03 plumtree.go:334: r1_node_1 - Added peer r1_node_5 to peers
REGION PT2025/07/23 08:12:03 tree.go:196: r1_node_1 - Processing onPeerUp peer: r1_node_5
REGION PT2025/07/23 08:12:03 tree.go:197: r1_node_1 - eager push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340}] lazy push peers []
REGION PT2025/07/23 08:12:03 tree.go:202: r1_node_1 - Added peer r1_node_5 to eager push peers
REGION PT2025/07/23 08:12:03 tree.go:204: r1_node_1 - eager push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}] lazy push peers []
REGION HV2025/07/23 08:12:03 hyparview.go:214: try lock
REGION HV2025/07/23 08:12:03 msg_handlers.go:89: try lock
REGION HV2025/07/23 08:12:03 msg_handlers.go:100: received ForwardJoin message self r1_node_1 from r1_node_5 TTL 0
REGION HV2025/07/23 08:12:03 hyparview.go:214: try lock
REGION HV2025/07/23 08:12:03 msg_handlers.go:89: try lock
REGION HV2025/07/23 08:12:03 msg_handlers.go:100: received ForwardJoin message self r1_node_1 from r1_node_5 TTL 0
REGION HV2025/07/23 08:12:03 hyparview.go:214: try lock
REGION HV2025/07/23 08:12:03 msg_handlers.go:89: try lock
REGION HV2025/07/23 08:12:03 msg_handlers.go:100: received ForwardJoin message self r1_node_1 from r1_node_5 TTL 1
MONOCEROS2025/07/23 08:12:03 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:305: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:314: init aggregation {1753258323}
REGION PT2025/07/23 08:12:03 plumtree.go:140: try lock
REGION PT2025/07/23 08:12:03 plumtree.go:143: r1_node_1 - Gossiping message
REGION PT2025/07/23 08:12:03 tree.go:57: r1_node_1 - Gossiping message
MONOCEROS2025/07/23 08:12:03 monoceros.go:361: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:364: received aggregation msg {RN_r1_node_1 r1_node_1} A_REQ {"Timestamp":1753258323} from r1_node_1
MONOCEROS2025/07/23 08:12:03 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:250: try lock
2025/07/23 08:12:03 metrics get
2025/07/23 08:12:03 # HELP app_request_processing_time_seconds Average request processing time
# TYPE app_request_processing_time_seconds gauge
app_request_processing_time_seconds 0.256

# HELP app_memory_usage_bytes Current memory usage in bytes
# TYPE app_memory_usage_bytes gauge
app_memory_usage_bytes 512

# HELP app_cpu_load_ratio CPU load (0-1)
# TYPE app_cpu_load_ratio gauge
app_cpu_load_ratio 0.13

# HELP app_active_sessions Current active user sessions
# TYPE app_active_sessions gauge
app_active_sessions 42

# HELP app_queue_depth_pending_jobs Jobs waiting in queue
# TYPE app_queue_depth_pending_jobs gauge
app_queue_depth_pending_jobs 7

# HELP app_cache_hit_ratio Cache hit ratio
# TYPE app_cache_hit_ratio gauge
app_cache_hit_ratio 0.82

# HELP app_current_goroutines Goroutine count
# TYPE app_current_goroutines gauge
app_current_goroutines 33

# HELP app_last_backup_timestamp_seconds Unix timestamp of last successful backup
# TYPE app_last_backup_timestamp_seconds gauge
app_last_backup_timestamp_seconds 1.700000e+09

# HELP app_http_requests_total Total HTTP requests processed
# TYPE app_http_requests_total counter
app_http_requests_total 12890

# HELP app_errors_total Total errors encountered
# TYPE app_errors_total counter
app_errors_total 17

# EOF

MONOCEROS2025/07/23 08:12:03 metric.go:83: metrics received
MONOCEROS2025/07/23 08:12:03 metric.go:84: # HELP app_request_processing_time_seconds Average request processing time
# TYPE app_request_processing_time_seconds gauge
app_request_processing_time_seconds 0.256

# HELP app_memory_usage_bytes Current memory usage in bytes
# TYPE app_memory_usage_bytes gauge
app_memory_usage_bytes 512

# HELP app_cpu_load_ratio CPU load (0-1)
# TYPE app_cpu_load_ratio gauge
app_cpu_load_ratio 0.13

# HELP app_active_sessions Current active user sessions
# TYPE app_active_sessions gauge
app_active_sessions 42

# HELP app_queue_depth_pending_jobs Jobs waiting in queue
# TYPE app_queue_depth_pending_jobs gauge
app_queue_depth_pending_jobs 7

# HELP app_cache_hit_ratio Cache hit ratio
# TYPE app_cache_hit_ratio gauge
app_cache_hit_ratio 0.82

# HELP app_current_goroutines Goroutine count
# TYPE app_current_goroutines gauge
app_current_goroutines 33

# HELP app_last_backup_timestamp_seconds Unix timestamp of last successful backup
# TYPE app_last_backup_timestamp_seconds gauge
app_last_backup_timestamp_seconds 1.700000e+09

# HELP app_http_requests_total Total HTTP requests processed
# TYPE app_http_requests_total counter
app_http_requests_total 12890

# HELP app_errors_total Total errors encountered
# TYPE app_errors_total counter
app_errors_total 17

# EOF

MONOCEROS2025/07/23 08:12:03 aggregation.go:228: get node metrics
MONOCEROS2025/07/23 08:12:03 aggregation.go:230: { {app_memory_usage_bytes map[]} 3 {total_app_memory_usage_bytes map[func:sum]}}
MONOCEROS2025/07/23 08:12:03 aggregation.go:232: [512]
MONOCEROS2025/07/23 08:12:03 aggregation.go:235: &{{total_app_memory_usage_bytes map[func:sum]} {512}}
MONOCEROS2025/07/23 08:12:03 aggregation.go:240: local [{{total_app_memory_usage_bytes map[func:sum]} {512}}]
MONOCEROS2025/07/23 08:12:03 aggregation.go:230: { {app_memory_usage_bytes map[]} 4 {avg_app_memory_usage_bytes map[func:avg]}}
MONOCEROS2025/07/23 08:12:03 aggregation.go:232: [512]
MONOCEROS2025/07/23 08:12:03 aggregation.go:235: &{{avg_app_memory_usage_bytes map[func:avg]} {512 1}}
MONOCEROS2025/07/23 08:12:03 aggregation.go:240: local [{{total_app_memory_usage_bytes map[func:sum]} {512}} {{avg_app_memory_usage_bytes map[func:avg]} {512 1}}]
MONOCEROS2025/07/23 08:12:03 aggregation.go:230: { {avg_app_memory_usage_bytes map[]} 4 {avg_app_memory_usage_bytes map[global:y]}}
MONOCEROS2025/07/23 08:12:03 aggregation.go:232: []
MONOCEROS2025/07/23 08:12:03 aggregation.go:235: <nil>
MONOCEROS2025/07/23 08:12:03 aggregation.go:230: { {total_app_memory_usage_bytes map[]} 3 {total_app_memory_usage_bytes map[global:y]}}
MONOCEROS2025/07/23 08:12:03 aggregation.go:232: []
MONOCEROS2025/07/23 08:12:03 aggregation.go:235: <nil>
REGION PT2025/07/23 08:12:03 plumtree.go:226: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:673: children to send req [{r1_node_2 r1_node_2:6001} {r1_node_3 r1_node_3:6001} {r1_node_4 r1_node_4:6001} {r1_node_5 r1_node_5:6001}]
REGION PT2025/07/23 08:12:03 plumtree.go:205: try lock
REGION PT2025/07/23 08:12:03 plumtree.go:208: r1_node_1 - Get parent [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
MONOCEROS2025/07/23 08:12:03 monoceros.go:674: has parent false
REGION PT2025/07/23 08:12:03 tree.go:60: try lock
REGION PT2025/07/23 08:12:03 tree.go:88: r1_node_1 - Eager push - sending
REGION PT2025/07/23 08:12:03 tree.go:91: r1_node_1 - peer {{r1_node_2 r1_node_2:6001} 0xc0000a6700}
REGION PT2025/07/23 08:12:03 tree.go:95: r1_node_1 - Sending gossip msg to peer: r1_node_2
REGION PT2025/07/23 08:12:03 tree.go:91: r1_node_1 - peer {{r1_node_3 r1_node_3:6001} 0xc00013e000}
REGION PT2025/07/23 08:12:03 tree.go:95: r1_node_1 - Sending gossip msg to peer: r1_node_3
REGION PT2025/07/23 08:12:03 tree.go:91: r1_node_1 - peer {{r1_node_4 r1_node_4:6001} 0xc000242340}
REGION PT2025/07/23 08:12:03 tree.go:95: r1_node_1 - Sending gossip msg to peer: r1_node_4
REGION PT2025/07/23 08:12:03 tree.go:91: r1_node_1 - peer {{r1_node_5 r1_node_5:6001} 0xc0002423c0}
REGION PT2025/07/23 08:12:03 tree.go:95: r1_node_1 - Sending gossip msg to peer: r1_node_5
REGION PT2025/07/23 08:12:03 tree.go:114: r1_node_1 - Lazy push - sending
REGION PT2025/07/23 08:12:03 tree.go:70: r1_node_1 - Message gossiped successfully
REGION HV2025/07/23 08:12:03 hyparview.go:214: try lock
REGION PT2025/07/23 08:12:03 plumtree.go:277: try lock
REGION PT2025/07/23 08:12:03 plumtree.go:279: r1_node_1 - Custom message handler invoked
REGION PT2025/07/23 08:12:03 plumtree.go:281: r1_node_1 - sender &{192.168.240.3:43200 0xc000068200 0xc000094ee0 0xc000094f50 0x683d00 0xc0000a34a0}
REGION PT2025/07/23 08:12:03 plumtree.go:295: r1_node_1 - received from r1_node_2
REGION PT2025/07/23 08:12:03 plumtree.go:304: r1_node_1 - Received message in subscription handler [1 123 34 77 101 116 97 100 97 116 97 34 58 123 34 73 100 34 58 34 82 78 95 114 49 95 110 111 100 101 95 49 34 44 34 83 99 111 114 101 34 58 34 114 49 95 110 111 100 101 95 49 34 125 44 34 77 115 103 84 121 112 101 34 58 34 65 66 79 82 84 95 82 69 83 80 34 44 34 77 115 103 34 58 34 101 121 74 85 97 87 49 108 99 51 82 104 98 88 65 105 79 106 69 51 78 84 77 121 78 84 103 122 77 106 78 57 34 44 34 77 115 103 73 100 34 58 34 50 113 76 90 51 69 80 84 100 104 48 61 34 44 34 82 111 117 110 100 34 58 48 125]
REGION PT2025/07/23 08:12:03 plumtree.go:305: try lock
REGION PT2025/07/23 08:12:03 msg_handlers.go:136: r1_node_1 - Processing direct message
MONOCEROS2025/07/23 08:12:03 monoceros.go:394: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:397: received direct msg {RN_r1_node_1 r1_node_1} ABORT_RESP {"Timestamp":1753258323} from r1_node_2
MONOCEROS2025/07/23 08:12:03 monoceros.go:692: complete aggregation req &{RN 0xc000034200 1753258323 0 0xc000094bd0 0xc000324000 0 [] true 0x78c980 map[level:region regionID:r1]} {RN_r1_node_1 r1_node_1}
REGION HV2025/07/23 08:12:03 hyparview.go:214: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:693: &{RN_r1_node_1 r1_node_1}
REGION PT2025/07/23 08:12:03 plumtree.go:277: try lock
REGION PT2025/07/23 08:12:03 plumtree.go:279: r1_node_1 - Custom message handler invoked
REGION PT2025/07/23 08:12:03 plumtree.go:281: r1_node_1 - sender &{192.168.240.4:6001 0xc000138000 0xc00013c000 0xc00013c070 0x683d00 0xc0000a34a0}
REGION PT2025/07/23 08:12:03 plumtree.go:295: r1_node_1 - received from r1_node_3
REGION HV2025/07/23 08:12:03 hyparview.go:214: try lock
REGION PT2025/07/23 08:12:03 plumtree.go:277: try lock
REGION PT2025/07/23 08:12:03 plumtree.go:279: r1_node_1 - Custom message handler invoked
REGION PT2025/07/23 08:12:03 plumtree.go:281: r1_node_1 - sender &{192.168.240.5:6001 0xc000236070 0xc000240540 0xc0002405b0 0x683d00 0xc0000a34a0}
REGION PT2025/07/23 08:12:03 plumtree.go:295: r1_node_1 - received from r1_node_4
MONOCEROS2025/07/23 08:12:03 monoceros.go:694: RN_r1_node_1
REGION PT2025/07/23 08:12:03 plumtree.go:205: try lock
REGION PT2025/07/23 08:12:03 plumtree.go:208: r1_node_1 - Get parent [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
REGION PT2025/07/23 08:12:03 plumtree.go:304: r1_node_1 - Received message in subscription handler [1 123 34 77 101 116 97 100 97 116 97 34 58 123 34 73 100 34 58 34 82 78 95 114 49 95 110 111 100 101 95 49 34 44 34 83 99 111 114 101 34 58 34 114 49 95 110 111 100 101 95 49 34 125 44 34 77 115 103 84 121 112 101 34 58 34 65 66 79 82 84 95 82 69 83 80 34 44 34 77 115 103 34 58 34 101 121 74 85 97 87 49 108 99 51 82 104 98 88 65 105 79 106 69 51 78 84 77 121 78 84 103 122 77 106 78 57 34 44 34 77 115 103 73 100 34 58 34 103 102 53 102 51 68 68 69 114 76 119 61 34 44 34 82 111 117 110 100 34 58 48 125]
REGION PT2025/07/23 08:12:03 plumtree.go:305: try lock
REGION PT2025/07/23 08:12:03 msg_handlers.go:136: r1_node_1 - Processing direct message
REGION PT2025/07/23 08:12:03 plumtree.go:304: r1_node_1 - Received message in subscription handler [1 123 34 77 101 116 97 100 97 116 97 34 58 123 34 73 100 34 58 34 82 78 95 114 49 95 110 111 100 101 95 49 34 44 34 83 99 111 114 101 34 58 34 114 49 95 110 111 100 101 95 49 34 125 44 34 77 115 103 84 121 112 101 34 58 34 65 66 79 82 84 95 82 69 83 80 34 44 34 77 115 103 34 58 34 101 121 74 85 97 87 49 108 99 51 82 104 98 88 65 105 79 106 69 51 78 84 77 121 78 84 103 122 77 106 78 57 34 44 34 77 115 103 73 100 34 58 34 121 65 84 101 72 77 49 118 114 89 99 61 34 44 34 82 111 117 110 100 34 58 48 125]
REGION PT2025/07/23 08:12:03 plumtree.go:305: try lock
REGION PT2025/07/23 08:12:03 msg_handlers.go:136: r1_node_1 - Processing direct message
MONOCEROS2025/07/23 08:12:03 monoceros.go:725: req done
MONOCEROS2025/07/23 08:12:03 monoceros.go:727: should destroy local tree
REGION PT2025/07/23 08:12:03 plumtree.go:118: destroying tree {RN_r1_node_1 r1_node_1}
REGION PT2025/07/23 08:12:03 plumtree.go:119: try lock
REGION PT2025/07/23 08:12:03 plumtree.go:125: r1_node_1 - trees map[RN_r1_node_1:0xc000326000]
MONOCEROS2025/07/23 08:12:03 monoceros.go:216: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:219: clean up tree RN {RN_r1_node_1 r1_node_1}
MONOCEROS2025/07/23 08:12:03 monoceros.go:220: active requests
MONOCEROS2025/07/23 08:12:03 monoceros.go:231: active requests
MONOCEROS2025/07/23 08:12:03 monoceros.go:242: local tree removed
MONOCEROS2025/07/23 08:12:03 monoceros.go:864: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:867: tree destroyed in RN, should leave RRN {RN_r1_node_1 r1_node_1}
MONOCEROS2025/07/23 08:12:03 monoceros.go:394: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:394: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:882: dosao do leave
RRN PT2025/07/23 08:12:03 plumtree.go:80: pt leave
RRN HV2025/07/23 08:12:03 hyparview.go:131: r1_node_1 already left the network
RRN PT2025/07/23 08:12:03 plumtree.go:82: pt left
MONOCEROS2025/07/23 08:12:03 monoceros.go:884: prosao leave
MONOCEROS2025/07/23 08:12:03 monoceros.go:899: sending rrn update [1 123 34 74 111 105 110 101 100 34 58 102 97 108 115 101 44 34 78 111 100 101 73 110 102 111 34 58 123 34 73 68 34 58 34 114 49 95 110 111 100 101 95 49 34 44 34 76 105 115 116 101 110 65 100 100 114 101 115 115 34 58 34 114 49 95 110 111 100 101 95 49 58 56 48 48 49 34 125 44 34 82 101 103 105 111 110 34 58 34 114 49 34 125]
REGION HV2025/07/23 08:12:03 hyparview.go:214: try lock
REGION PT2025/07/23 08:12:03 plumtree.go:277: try lock
REGION PT2025/07/23 08:12:03 plumtree.go:279: r1_node_1 - Custom message handler invoked
REGION PT2025/07/23 08:12:03 plumtree.go:281: r1_node_1 - sender &{192.168.240.6:58506 0xc000236078 0xc000240620 0xc000240690 0x683d00 0xc0000a34a0}
REGION PT2025/07/23 08:12:03 plumtree.go:295: r1_node_1 - received from r1_node_5
REGION PT2025/07/23 08:12:03 plumtree.go:304: r1_node_1 - Received message in subscription handler [1 123 34 77 101 116 97 100 97 116 97 34 58 123 34 73 100 34 58 34 82 78 95 114 49 95 110 111 100 101 95 49 34 44 34 83 99 111 114 101 34 58 34 114 49 95 110 111 100 101 95 49 34 125 44 34 77 115 103 84 121 112 101 34 58 34 65 66 79 82 84 95 82 69 83 80 34 44 34 77 115 103 34 58 34 101 121 74 85 97 87 49 108 99 51 82 104 98 88 65 105 79 106 69 51 78 84 77 121 78 84 103 122 77 106 78 57 34 44 34 77 115 103 73 100 34 58 34 77 87 106 102 100 98 105 52 104 73 52 61 34 44 34 82 111 117 110 100 34 58 48 125]
REGION PT2025/07/23 08:12:03 plumtree.go:305: try lock
REGION PT2025/07/23 08:12:03 msg_handlers.go:136: r1_node_1 - Processing direct message
MONOCEROS2025/07/23 08:12:03 monoceros.go:424: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:397: received direct msg {RN_r1_node_1 r1_node_1} ABORT_RESP {"Timestamp":1753258323} from r1_node_3
MONOCEROS2025/07/23 08:12:03 monoceros.go:615: could not find active request for response {1753258323} active requests []
MONOCEROS2025/07/23 08:12:03 monoceros.go:397: received direct msg {RN_r1_node_1 r1_node_1} ABORT_RESP {"Timestamp":1753258323} from r1_node_4
MONOCEROS2025/07/23 08:12:03 monoceros.go:615: could not find active request for response {1753258323} active requests []
MONOCEROS2025/07/23 08:12:03 monoceros.go:427: received global network msg [1 123 34 74 111 105 110 101 100 34 58 102 97 108 115 101 44 34 78 111 100 101 73 110 102 111 34 58 123 34 73 68 34 58 34 114 49 95 110 111 100 101 95 49 34 44 34 76 105 115 116 101 110 65 100 100 114 101 115 115 34 58 34 114 49 95 110 111 100 101 95 49 58 56 48 48 49 34 125 44 34 82 101 103 105 111 110 34 58 34 114 49 34 125]
MONOCEROS2025/07/23 08:12:03 monoceros.go:430: received regional root update
MONOCEROS2025/07/23 08:12:03 monoceros.go:452: map[]
MONOCEROS2025/07/23 08:12:03 monoceros.go:453: map[]
GLOBAL HV2025/07/23 08:12:03 hyparview.go:156: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:394: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:902: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:397: received direct msg {RN_r1_node_1 r1_node_1} ABORT_RESP {"Timestamp":1753258323} from r1_node_5
MONOCEROS2025/07/23 08:12:03 monoceros.go:615: could not find active request for response {1753258323} active requests []
REGION PT2025/07/23 08:12:03 plumtree.go:129: try lock
REGION PT2025/07/23 08:12:03 plumtree.go:133: r1_node_1 - trees map[RN_r1_node_1:0xc000326000]
MONOCEROS2025/07/23 08:12:03 monoceros.go:730: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:03 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:03 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:03 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
MONOCEROS2025/07/23 08:12:03 monoceros.go:266: peers num 4 now time 1753258323 expected aggregation time 1.7532583282e+09
MONOCEROS2025/07/23 08:12:03 monoceros.go:803: local rn tree destroyed in the meantime, should not join rrn
MONOCEROS2025/07/23 08:12:03 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:03 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:03 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:03 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
MONOCEROS2025/07/23 08:12:03 monoceros.go:266: peers num 4 now time 1753258323 expected aggregation time 1.7532583282e+09
MONOCEROS2025/07/23 08:12:03 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:03 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:03 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:03 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
MONOCEROS2025/07/23 08:12:03 monoceros.go:266: peers num 4 now time 1753258323 expected aggregation time 1.7532583282e+09
MONOCEROS2025/07/23 08:12:03 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:03 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:03 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:03 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
MONOCEROS2025/07/23 08:12:03 monoceros.go:266: peers num 4 now time 1753258323 expected aggregation time 1.7532583282e+09
MONOCEROS2025/07/23 08:12:03 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:03 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:03 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:03 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:03 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
MONOCEROS2025/07/23 08:12:03 monoceros.go:266: peers num 4 now time 1753258323 expected aggregation time 1.7532583282e+09
MONOCEROS2025/07/23 08:12:04 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:04 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:04 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:04 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:04 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:04 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
MONOCEROS2025/07/23 08:12:04 monoceros.go:266: peers num 4 now time 1753258324 expected aggregation time 1.7532583282e+09
MONOCEROS2025/07/23 08:12:04 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:04 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:04 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:04 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:04 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:04 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
MONOCEROS2025/07/23 08:12:04 monoceros.go:266: peers num 4 now time 1753258324 expected aggregation time 1.7532583282e+09
GLOBAL HV2025/07/23 08:12:04 hyparview.go:214: try lock
GLOBAL HV2025/07/23 08:12:04 msg_handlers.go:89: try lock
GLOBAL HV2025/07/23 08:12:04 msg_handlers.go:100: received ForwardJoin message self r1_node_1 from r1_node_6 TTL 1
GLOBAL HV2025/07/23 08:12:04 hyparview.go:214: try lock
GLOBAL HV2025/07/23 08:12:04 msg_handlers.go:89: try lock
GLOBAL HV2025/07/23 08:12:04 msg_handlers.go:100: received ForwardJoin message self r1_node_1 from r1_node_6 TTL 1
REGION HV2025/07/23 08:12:04 hyparview.go:214: try lock
REGION HV2025/07/23 08:12:04 msg_handlers.go:89: try lock
REGION HV2025/07/23 08:12:04 msg_handlers.go:100: received ForwardJoin message self r1_node_1 from r1_node_6 TTL 3
REGION HV2025/07/23 08:12:04 msg_handlers.go:143: added peer to passive view via forward join peerID r1_node_6
REGION HV2025/07/23 08:12:04 hyparview.go:214: try lock
REGION HV2025/07/23 08:12:04 msg_handlers.go:89: try lock
REGION HV2025/07/23 08:12:04 msg_handlers.go:100: received ForwardJoin message self r1_node_1 from r1_node_6 TTL 1
REGION HV2025/07/23 08:12:04 hyparview.go:214: try lock
REGION HV2025/07/23 08:12:04 msg_handlers.go:89: try lock
REGION HV2025/07/23 08:12:04 msg_handlers.go:100: received ForwardJoin message self r1_node_1 from r1_node_6 TTL 2
MONOCEROS2025/07/23 08:12:04 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:04 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:04 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:04 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:04 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:04 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
MONOCEROS2025/07/23 08:12:04 monoceros.go:266: peers num 4 now time 1753258324 expected aggregation time 1.7532583282e+09
MONOCEROS2025/07/23 08:12:04 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:04 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:04 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:04 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:04 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:04 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
MONOCEROS2025/07/23 08:12:04 monoceros.go:266: peers num 4 now time 1753258324 expected aggregation time 1.7532583282e+09
MONOCEROS2025/07/23 08:12:04 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:04 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:04 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:04 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:04 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:04 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
MONOCEROS2025/07/23 08:12:04 monoceros.go:266: peers num 4 now time 1753258324 expected aggregation time 1.7532583282e+09
REGION HV2025/07/23 08:12:04 hyparview.go:214: try lock
REGION PT2025/07/23 08:12:04 plumtree.go:277: try lock
REGION PT2025/07/23 08:12:04 plumtree.go:279: r1_node_1 - Custom message handler invoked
REGION PT2025/07/23 08:12:04 plumtree.go:281: r1_node_1 - sender &{192.168.240.3:43200 0xc000068200 0xc000094ee0 0xc000094f50 0x683d00 0xc0000a34a0}
REGION PT2025/07/23 08:12:04 plumtree.go:295: r1_node_1 - received from r1_node_2
REGION PT2025/07/23 08:12:04 plumtree.go:304: r1_node_1 - Received message in subscription handler [0 123 34 77 101 116 97 100 97 116 97 34 58 123 34 73 100 34 58 34 82 78 95 114 49 95 110 111 100 101 95 50 34 44 34 83 99 111 114 101 34 58 34 114 49 95 110 111 100 101 95 50 34 125 44 34 77 115 103 84 121 112 101 34 58 34 65 95 82 69 81 34 44 34 77 115 103 34 58 34 101 121 74 85 97 87 49 108 99 51 82 104 98 88 65 105 79 106 69 51 78 84 77 121 78 84 103 122 77 106 82 57 34 44 34 77 115 103 73 100 34 58 34 86 103 48 97 122 113 77 80 111 86 56 61 34 44 34 82 111 117 110 100 34 58 49 125]
REGION PT2025/07/23 08:12:04 plumtree.go:305: try lock
REGION PT2025/07/23 08:12:04 msg_handlers.go:21: r1_node_1 - tree with id=RN_r1_node_2 not found
REGION PT2025/07/23 08:12:04 msg_handlers.go:23: r1_node_1 - tree created RN_r1_node_2
REGION PT2025/07/23 08:12:04 msg_handlers.go:97: r1_node_1 - Processing gossip message
REGION PT2025/07/23 08:12:04 msg_handlers.go:101: r1_node_1 - message [86 13 26 206 163 15 161 95] received for the first time add sender to eager push peers {r1_node_2 r1_node_2:6001}
MONOCEROS2025/07/23 08:12:04 monoceros.go:361: try lock
MONOCEROS2025/07/23 08:12:04 monoceros.go:364: received aggregation msg {RN_r1_node_2 r1_node_2} A_REQ {"Timestamp":1753258324} from r1_node_2
MONOCEROS2025/07/23 08:12:04 monoceros.go:788: try lock
2025/07/23 08:12:04 metrics get
2025/07/23 08:12:04 # HELP app_request_processing_time_seconds Average request processing time
# TYPE app_request_processing_time_seconds gauge
app_request_processing_time_seconds 0.256

# HELP app_memory_usage_bytes Current memory usage in bytes
# TYPE app_memory_usage_bytes gauge
app_memory_usage_bytes 512

# HELP app_cpu_load_ratio CPU load (0-1)
# TYPE app_cpu_load_ratio gauge
app_cpu_load_ratio 0.13

# HELP app_active_sessions Current active user sessions
# TYPE app_active_sessions gauge
app_active_sessions 42

# HELP app_queue_depth_pending_jobs Jobs waiting in queue
# TYPE app_queue_depth_pending_jobs gauge
app_queue_depth_pending_jobs 7

# HELP app_cache_hit_ratio Cache hit ratio
# TYPE app_cache_hit_ratio gauge
app_cache_hit_ratio 0.82

# HELP app_current_goroutines Goroutine count
# TYPE app_current_goroutines gauge
app_current_goroutines 33

# HELP app_last_backup_timestamp_seconds Unix timestamp of last successful backup
# TYPE app_last_backup_timestamp_seconds gauge
app_last_backup_timestamp_seconds 1.700000e+09

# HELP app_http_requests_total Total HTTP requests processed
# TYPE app_http_requests_total counter
app_http_requests_total 12890

# HELP app_errors_total Total errors encountered
# TYPE app_errors_total counter
app_errors_total 17

# EOF

MONOCEROS2025/07/23 08:12:04 metric.go:83: metrics received
MONOCEROS2025/07/23 08:12:04 metric.go:84: # HELP app_request_processing_time_seconds Average request processing time
# TYPE app_request_processing_time_seconds gauge
app_request_processing_time_seconds 0.256

# HELP app_memory_usage_bytes Current memory usage in bytes
# TYPE app_memory_usage_bytes gauge
app_memory_usage_bytes 512

# HELP app_cpu_load_ratio CPU load (0-1)
# TYPE app_cpu_load_ratio gauge
app_cpu_load_ratio 0.13

# HELP app_active_sessions Current active user sessions
# TYPE app_active_sessions gauge
app_active_sessions 42

# HELP app_queue_depth_pending_jobs Jobs waiting in queue
# TYPE app_queue_depth_pending_jobs gauge
app_queue_depth_pending_jobs 7

# HELP app_cache_hit_ratio Cache hit ratio
# TYPE app_cache_hit_ratio gauge
app_cache_hit_ratio 0.82

# HELP app_current_goroutines Goroutine count
# TYPE app_current_goroutines gauge
app_current_goroutines 33

# HELP app_last_backup_timestamp_seconds Unix timestamp of last successful backup
# TYPE app_last_backup_timestamp_seconds gauge
app_last_backup_timestamp_seconds 1.700000e+09

# HELP app_http_requests_total Total HTTP requests processed
# TYPE app_http_requests_total counter
app_http_requests_total 12890

# HELP app_errors_total Total errors encountered
# TYPE app_errors_total counter
app_errors_total 17

# EOF

GLOBAL HV2025/07/23 08:12:04 hyparview.go:214: try lock
2025/07/23 08:12:04 received gossip msg [84 153 128 104 0 0 0 0 1 123 34 74 111 105 110 101 100 34 58 102 97 108 115 101 44 34 78 111 100 101 73 110 102 111 34 58 123 34 73 68 34 58 34 114 49 95 110 111 100 101 95 50 34 44 34 76 105 115 116 101 110 65 100 100 114 101 115 115 34 58 34 114 49 95 110 111 100 101 95 50 58 56 48 48 49 34 125 44 34 82 101 103 105 111 110 34 58 34 114 49 34 125]
2025/07/23 08:12:04 Received: [1 123 34 74 111 105 110 101 100 34 58 102 97 108 115 101 44 34 78 111 100 101 73 110 102 111 34 58 123 34 73 68 34 58 34 114 49 95 110 111 100 101 95 50 34 44 34 76 105 115 116 101 110 65 100 100 114 101 115 115 34 58 34 114 49 95 110 111 100 101 95 50 58 56 48 48 49 34 125 44 34 82 101 103 105 111 110 34 58 34 114 49 34 125]
MONOCEROS2025/07/23 08:12:04 monoceros.go:424: try lock
MONOCEROS2025/07/23 08:12:04 aggregation.go:228: get node metrics
MONOCEROS2025/07/23 08:12:04 aggregation.go:230: { {app_memory_usage_bytes map[]} 3 {total_app_memory_usage_bytes map[func:sum]}}
MONOCEROS2025/07/23 08:12:04 aggregation.go:232: [512]
MONOCEROS2025/07/23 08:12:04 aggregation.go:235: &{{total_app_memory_usage_bytes map[func:sum]} {512}}
MONOCEROS2025/07/23 08:12:04 aggregation.go:240: local [{{total_app_memory_usage_bytes map[func:sum]} {512}}]
MONOCEROS2025/07/23 08:12:04 aggregation.go:230: { {app_memory_usage_bytes map[]} 4 {avg_app_memory_usage_bytes map[func:avg]}}
MONOCEROS2025/07/23 08:12:04 aggregation.go:232: [512]
MONOCEROS2025/07/23 08:12:04 aggregation.go:235: &{{avg_app_memory_usage_bytes map[func:avg]} {512 1}}
MONOCEROS2025/07/23 08:12:04 aggregation.go:240: local [{{total_app_memory_usage_bytes map[func:sum]} {512}} {{avg_app_memory_usage_bytes map[func:avg]} {512 1}}]
MONOCEROS2025/07/23 08:12:04 aggregation.go:230: { {avg_app_memory_usage_bytes map[]} 4 {avg_app_memory_usage_bytes map[global:y]}}
MONOCEROS2025/07/23 08:12:04 aggregation.go:232: []
MONOCEROS2025/07/23 08:12:04 aggregation.go:235: <nil>
MONOCEROS2025/07/23 08:12:04 aggregation.go:230: { {total_app_memory_usage_bytes map[]} 3 {total_app_memory_usage_bytes map[global:y]}}
MONOCEROS2025/07/23 08:12:04 aggregation.go:232: []
MONOCEROS2025/07/23 08:12:04 aggregation.go:235: <nil>
REGION PT2025/07/23 08:12:04 plumtree.go:226: try lock
MONOCEROS2025/07/23 08:12:04 monoceros.go:673: children to send req [{r1_node_3 r1_node_3:6001} {r1_node_4 r1_node_4:6001} {r1_node_5 r1_node_5:6001}]
REGION PT2025/07/23 08:12:04 plumtree.go:205: try lock
REGION PT2025/07/23 08:12:04 plumtree.go:208: r1_node_1 - Get parent [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
MONOCEROS2025/07/23 08:12:04 monoceros.go:674: has parent true
REGION PT2025/07/23 08:12:04 msg_handlers.go:112: try lock
REGION PT2025/07/23 08:12:04 tree.go:88: r1_node_1 - Eager push - sending
REGION PT2025/07/23 08:12:04 tree.go:91: r1_node_1 - peer {{r1_node_2 r1_node_2:6001} 0xc0000a6700}
REGION PT2025/07/23 08:12:04 tree.go:91: r1_node_1 - peer {{r1_node_3 r1_node_3:6001} 0xc00013e000}
REGION PT2025/07/23 08:12:04 tree.go:95: r1_node_1 - Sending gossip msg to peer: r1_node_3
REGION PT2025/07/23 08:12:04 tree.go:91: r1_node_1 - peer {{r1_node_4 r1_node_4:6001} 0xc000242340}
REGION PT2025/07/23 08:12:04 tree.go:95: r1_node_1 - Sending gossip msg to peer: r1_node_4
MONOCEROS2025/07/23 08:12:04 monoceros.go:791: tree constructed in RN, should join RRN {RN_r1_node_2 r1_node_2}
MONOCEROS2025/07/23 08:12:04 monoceros.go:793: should not
MONOCEROS2025/07/23 08:12:04 monoceros.go:427: received global network msg [1 123 34 74 111 105 110 101 100 34 58 102 97 108 115 101 44 34 78 111 100 101 73 110 102 111 34 58 123 34 73 68 34 58 34 114 49 95 110 111 100 101 95 50 34 44 34 76 105 115 116 101 110 65 100 100 114 101 115 115 34 58 34 114 49 95 110 111 100 101 95 50 58 56 48 48 49 34 125 44 34 82 101 103 105 111 110 34 58 34 114 49 34 125]
MONOCEROS2025/07/23 08:12:04 monoceros.go:430: received regional root update
MONOCEROS2025/07/23 08:12:04 monoceros.go:452: map[]
MONOCEROS2025/07/23 08:12:04 monoceros.go:453: map[]
REGION HV2025/07/23 08:12:04 hyparview.go:214: try lock
GLOBAL HV2025/07/23 08:12:04 hyparview.go:156: try lock
REGION PT2025/07/23 08:12:04 tree.go:91: r1_node_1 - peer {{r1_node_5 r1_node_5:6001} 0xc0002423c0}
REGION PT2025/07/23 08:12:04 tree.go:95: r1_node_1 - Sending gossip msg to peer: r1_node_5
REGION PT2025/07/23 08:12:04 tree.go:114: r1_node_1 - Lazy push - sending
REGION PT2025/07/23 08:12:04 plumtree.go:277: try lock
REGION PT2025/07/23 08:12:04 plumtree.go:279: r1_node_1 - Custom message handler invoked
REGION PT2025/07/23 08:12:04 plumtree.go:281: r1_node_1 - sender &{192.168.240.4:6001 0xc000138000 0xc00013c000 0xc00013c070 0x683d00 0xc0000a34a0}
REGION PT2025/07/23 08:12:04 plumtree.go:295: r1_node_1 - received from r1_node_3
REGION PT2025/07/23 08:12:04 plumtree.go:304: r1_node_1 - Received message in subscription handler [2 123 34 77 101 116 97 100 97 116 97 34 58 123 34 73 100 34 58 34 82 78 95 114 49 95 110 111 100 101 95 50 34 44 34 83 99 111 114 101 34 58 34 114 49 95 110 111 100 101 95 50 34 125 125]
REGION PT2025/07/23 08:12:04 plumtree.go:305: try lock
REGION PT2025/07/23 08:12:04 msg_handlers.go:142: r1_node_1 - Processing prune message from peer: r1_node_3
REGION PT2025/07/23 08:12:04 msg_handlers.go:143: r1_node_1 - eager push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}] lazy push peers []
REGION PT2025/07/23 08:12:04 msg_handlers.go:145: r1_node_1 - eager push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}] lazy push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000}]
REGION HV2025/07/23 08:12:04 hyparview.go:214: try lock
REGION PT2025/07/23 08:12:04 plumtree.go:277: try lock
REGION PT2025/07/23 08:12:04 plumtree.go:279: r1_node_1 - Custom message handler invoked
REGION PT2025/07/23 08:12:04 plumtree.go:281: r1_node_1 - sender &{192.168.240.5:6001 0xc000236070 0xc000240540 0xc0002405b0 0x683d00 0xc0000a34a0}
REGION PT2025/07/23 08:12:04 plumtree.go:295: r1_node_1 - received from r1_node_4
REGION PT2025/07/23 08:12:04 plumtree.go:304: r1_node_1 - Received message in subscription handler [2 123 34 77 101 116 97 100 97 116 97 34 58 123 34 73 100 34 58 34 82 78 95 114 49 95 110 111 100 101 95 50 34 44 34 83 99 111 114 101 34 58 34 114 49 95 110 111 100 101 95 50 34 125 125]
REGION PT2025/07/23 08:12:04 plumtree.go:305: try lock
GLOBAL HV2025/07/23 08:12:04 hyparview.go:214: try lock
2025/07/23 08:12:04 received gossip msg [84 153 128 104 0 0 0 0 1 123 34 74 111 105 110 101 100 34 58 102 97 108 115 101 44 34 78 111 100 101 73 110 102 111 34 58 123 34 73 68 34 58 34 114 49 95 110 111 100 101 95 50 34 44 34 76 105 115 116 101 110 65 100 100 114 101 115 115 34 58 34 114 49 95 110 111 100 101 95 50 58 56 48 48 49 34 125 44 34 82 101 103 105 111 110 34 58 34 114 49 34 125]
2025/07/23 08:12:04 msg already seen: [84 153 128 104 0 0 0 0 1 123 34 74 111 105 110 101 100 34 58 102 97 108 115 101 44 34 78 111 100 101 73 110 102 111 34 58 123 34 73 68 34 58 34 114 49 95 110 111 100 101 95 50 34 44 34 76 105 115 116 101 110 65 100 100 114 101 115 115 34 58 34 114 49 95 110 111 100 101 95 50 58 56 48 48 49 34 125 44 34 82 101 103 105 111 110 34 58 34 114 49 34 125]
GLOBAL HV2025/07/23 08:12:04 hyparview.go:214: try lock
2025/07/23 08:12:04 received gossip msg [84 153 128 104 0 0 0 0 1 123 34 74 111 105 110 101 100 34 58 102 97 108 115 101 44 34 78 111 100 101 73 110 102 111 34 58 123 34 73 68 34 58 34 114 49 95 110 111 100 101 95 50 34 44 34 76 105 115 116 101 110 65 100 100 114 101 115 115 34 58 34 114 49 95 110 111 100 101 95 50 58 56 48 48 49 34 125 44 34 82 101 103 105 111 110 34 58 34 114 49 34 125]
2025/07/23 08:12:04 msg already seen: [84 153 128 104 0 0 0 0 1 123 34 74 111 105 110 101 100 34 58 102 97 108 115 101 44 34 78 111 100 101 73 110 102 111 34 58 123 34 73 68 34 58 34 114 49 95 110 111 100 101 95 50 34 44 34 76 105 115 116 101 110 65 100 100 114 101 115 115 34 58 34 114 49 95 110 111 100 101 95 50 58 56 48 48 49 34 125 44 34 82 101 103 105 111 110 34 58 34 114 49 34 125]
REGION PT2025/07/23 08:12:04 msg_handlers.go:142: r1_node_1 - Processing prune message from peer: r1_node_4
REGION PT2025/07/23 08:12:04 msg_handlers.go:143: r1_node_1 - eager push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}] lazy push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000}]
REGION PT2025/07/23 08:12:04 msg_handlers.go:145: r1_node_1 - eager push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}] lazy push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340}]
REGION HV2025/07/23 08:12:04 hyparview.go:214: try lock
REGION PT2025/07/23 08:12:04 plumtree.go:277: try lock
REGION PT2025/07/23 08:12:04 plumtree.go:279: r1_node_1 - Custom message handler invoked
REGION PT2025/07/23 08:12:04 plumtree.go:281: r1_node_1 - sender &{192.168.240.6:58506 0xc000236078 0xc000240620 0xc000240690 0x683d00 0xc0000a34a0}
REGION PT2025/07/23 08:12:04 plumtree.go:295: r1_node_1 - received from r1_node_5
REGION PT2025/07/23 08:12:04 plumtree.go:304: r1_node_1 - Received message in subscription handler [1 123 34 77 101 116 97 100 97 116 97 34 58 123 34 73 100 34 58 34 82 78 95 114 49 95 110 111 100 101 95 50 34 44 34 83 99 111 114 101 34 58 34 114 49 95 110 111 100 101 95 50 34 125 44 34 77 115 103 84 121 112 101 34 58 34 65 66 79 82 84 95 82 69 83 80 34 44 34 77 115 103 34 58 34 101 121 74 85 97 87 49 108 99 51 82 104 98 88 65 105 79 106 69 51 78 84 77 121 78 84 103 122 77 106 82 57 34 44 34 77 115 103 73 100 34 58 34 111 80 78 76 78 118 113 53 97 43 65 61 34 44 34 82 111 117 110 100 34 58 48 125]
REGION PT2025/07/23 08:12:04 plumtree.go:305: try lock
REGION PT2025/07/23 08:12:04 msg_handlers.go:136: r1_node_1 - Processing direct message
MONOCEROS2025/07/23 08:12:04 monoceros.go:394: try lock
MONOCEROS2025/07/23 08:12:04 monoceros.go:397: received direct msg {RN_r1_node_2 r1_node_2} ABORT_RESP {"Timestamp":1753258324} from r1_node_5
MONOCEROS2025/07/23 08:12:04 monoceros.go:692: complete aggregation req &{RN 0xc000034200 1753258324 0 0xc000094bd0 <nil> 0 [] true 0x78c980 map[level:region regionID:r1]} {RN_r1_node_2 r1_node_2}
MONOCEROS2025/07/23 08:12:04 monoceros.go:693: <nil>
MONOCEROS2025/07/23 08:12:04 monoceros.go:694: RN_r1_node_2
REGION PT2025/07/23 08:12:04 plumtree.go:205: try lock
REGION PT2025/07/23 08:12:04 plumtree.go:208: r1_node_1 - Get parent [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
MONOCEROS2025/07/23 08:12:04 monoceros.go:696: has parent
REGION PT2025/07/23 08:12:04 plumtree.go:165: try lock
REGION PT2025/07/23 08:12:04 plumtree.go:168: r1_node_1 - send to parent [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
REGION PT2025/07/23 08:12:04 plumtree.go:183: r1_node_1 - Sending message
REGION PT2025/07/23 08:12:04 tree.go:76: r1_node_1 - Sending direct message
REGION PT2025/07/23 08:12:04 tree.go:82: r1_node_1 - Message sent successfully
MONOCEROS2025/07/23 08:12:04 monoceros.go:719: try lock
MONOCEROS2025/07/23 08:12:04 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:04 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:04 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:04 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:04 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:04 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
MONOCEROS2025/07/23 08:12:04 monoceros.go:266: peers num 4 now time 1753258324 expected aggregation time 1.7532583292e+09
MONOCEROS2025/07/23 08:12:04 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:04 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:04 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:04 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:04 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
MONOCEROS2025/07/23 08:12:04 monoceros.go:266: peers num 4 now time 1753258324 expected aggregation time 1.7532583292e+09
MONOCEROS2025/07/23 08:12:04 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:04 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:04 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:04 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:04 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:04 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:04 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
MONOCEROS2025/07/23 08:12:04 monoceros.go:266: peers num 4 now time 1753258324 expected aggregation time 1.7532583292e+09
MONOCEROS2025/07/23 08:12:04 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:04 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:04 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:04 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:04 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
MONOCEROS2025/07/23 08:12:04 monoceros.go:266: peers num 4 now time 1753258324 expected aggregation time 1.7532583292e+09
MONOCEROS2025/07/23 08:12:04 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:04 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:04 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:04 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:04 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:04 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:04 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
MONOCEROS2025/07/23 08:12:04 monoceros.go:266: peers num 4 now time 1753258324 expected aggregation time 1.7532583292e+09
MONOCEROS2025/07/23 08:12:05 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:05 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:05 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
MONOCEROS2025/07/23 08:12:05 monoceros.go:266: peers num 4 now time 1753258325 expected aggregation time 1.7532583292e+09
MONOCEROS2025/07/23 08:12:05 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:05 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:05 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
MONOCEROS2025/07/23 08:12:05 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:266: peers num 4 now time 1753258325 expected aggregation time 1.7532583292e+09
MONOCEROS2025/07/23 08:12:05 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:05 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:05 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
MONOCEROS2025/07/23 08:12:05 monoceros.go:266: peers num 4 now time 1753258325 expected aggregation time 1.7532583292e+09
MONOCEROS2025/07/23 08:12:05 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:05 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:05 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
MONOCEROS2025/07/23 08:12:05 monoceros.go:266: peers num 4 now time 1753258325 expected aggregation time 1.7532583292e+09
MONOCEROS2025/07/23 08:12:05 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:05 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:05 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
MONOCEROS2025/07/23 08:12:05 monoceros.go:266: peers num 4 now time 1753258325 expected aggregation time 1.7532583292e+09
MONOCEROS2025/07/23 08:12:05 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:05 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:05 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
MONOCEROS2025/07/23 08:12:05 monoceros.go:266: peers num 4 now time 1753258325 expected aggregation time 1.7532583292e+09
GLOBAL HV2025/07/23 08:12:05 hyparview.go:214: try lock
GLOBAL HV2025/07/23 08:12:05 msg_handlers.go:89: try lock
GLOBAL HV2025/07/23 08:12:05 msg_handlers.go:100: received ForwardJoin message self r1_node_1 from r1_node_7 TTL 2
GLOBAL HV2025/07/23 08:12:05 hyparview.go:214: try lock
GLOBAL HV2025/07/23 08:12:05 msg_handlers.go:89: try lock
GLOBAL HV2025/07/23 08:12:05 msg_handlers.go:100: received ForwardJoin message self r1_node_1 from r1_node_7 TTL 1
REGION HV2025/07/23 08:12:05 hyparview.go:214: try lock
REGION HV2025/07/23 08:12:05 msg_handlers.go:89: try lock
REGION HV2025/07/23 08:12:05 msg_handlers.go:100: received ForwardJoin message self r1_node_1 from r1_node_7 TTL 3
REGION HV2025/07/23 08:12:05 msg_handlers.go:143: added peer to passive view via forward join peerID r1_node_7
REGION HV2025/07/23 08:12:05 hyparview.go:214: try lock
REGION HV2025/07/23 08:12:05 msg_handlers.go:89: try lock
REGION HV2025/07/23 08:12:05 msg_handlers.go:100: received ForwardJoin message self r1_node_1 from r1_node_7 TTL 2
REGION HV2025/07/23 08:12:05 hyparview.go:214: try lock
REGION HV2025/07/23 08:12:05 msg_handlers.go:89: try lock
REGION HV2025/07/23 08:12:05 msg_handlers.go:100: received ForwardJoin message self r1_node_1 from r1_node_7 TTL 0
REGION HV2025/07/23 08:12:05 msg_handlers.go:138: added peer to active view via forward join peerID r1_node_7 address r1_node_7:6001
REGION PT2025/07/23 08:12:05 plumtree.go:325: r1_node_1 - Processing onPeerUp peer: r1_node_7
REGION PT2025/07/23 08:12:05 plumtree.go:326: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:334: r1_node_1 - Added peer r1_node_7 to peers
REGION PT2025/07/23 08:12:05 tree.go:196: r1_node_1 - Processing onPeerUp peer: r1_node_7
REGION PT2025/07/23 08:12:05 tree.go:197: r1_node_1 - eager push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}] lazy push peers []
REGION PT2025/07/23 08:12:05 tree.go:202: r1_node_1 - Added peer r1_node_7 to eager push peers
REGION PT2025/07/23 08:12:05 tree.go:204: r1_node_1 - eager push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}] lazy push peers []
REGION PT2025/07/23 08:12:05 tree.go:196: r1_node_1 - Processing onPeerUp peer: r1_node_7
REGION PT2025/07/23 08:12:05 tree.go:197: r1_node_1 - eager push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}] lazy push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340}]
REGION PT2025/07/23 08:12:05 tree.go:202: r1_node_1 - Added peer r1_node_7 to eager push peers
REGION PT2025/07/23 08:12:05 tree.go:204: r1_node_1 - eager push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}] lazy push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340}]
MONOCEROS2025/07/23 08:12:05 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:05 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:05 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}]
MONOCEROS2025/07/23 08:12:05 monoceros.go:266: peers num 5 now time 1753258325 expected aggregation time 1.7532583292e+09
REGION HV2025/07/23 08:12:05 hyparview.go:214: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:277: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:279: r1_node_1 - Custom message handler invoked
REGION PT2025/07/23 08:12:05 plumtree.go:281: r1_node_1 - sender &{192.168.240.4:6001 0xc000138000 0xc00013c000 0xc00013c070 0x683d00 0xc0000a34a0}
REGION PT2025/07/23 08:12:05 plumtree.go:295: r1_node_1 - received from r1_node_3
REGION PT2025/07/23 08:12:05 plumtree.go:304: r1_node_1 - Received message in subscription handler [0 123 34 77 101 116 97 100 97 116 97 34 58 123 34 73 100 34 58 34 82 78 95 114 49 95 110 111 100 101 95 51 34 44 34 83 99 111 114 101 34 58 34 114 49 95 110 111 100 101 95 51 34 125 44 34 77 115 103 84 121 112 101 34 58 34 65 95 82 69 81 34 44 34 77 115 103 34 58 34 101 121 74 85 97 87 49 108 99 51 82 104 98 88 65 105 79 106 69 51 78 84 77 121 78 84 103 122 77 106 86 57 34 44 34 77 115 103 73 100 34 58 34 112 82 67 107 90 97 81 101 68 107 81 61 34 44 34 82 111 117 110 100 34 58 49 125]
REGION PT2025/07/23 08:12:05 plumtree.go:305: try lock
REGION PT2025/07/23 08:12:05 msg_handlers.go:21: r1_node_1 - tree with id=RN_r1_node_3 not found
REGION PT2025/07/23 08:12:05 msg_handlers.go:23: r1_node_1 - tree created RN_r1_node_3
REGION PT2025/07/23 08:12:05 msg_handlers.go:97: r1_node_1 - Processing gossip message
REGION PT2025/07/23 08:12:05 msg_handlers.go:101: r1_node_1 - message [165 16 164 101 164 30 14 68] received for the first time add sender to eager push peers {r1_node_3 r1_node_3:6001}
MONOCEROS2025/07/23 08:12:05 monoceros.go:361: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:364: received aggregation msg {RN_r1_node_3 r1_node_3} A_REQ {"Timestamp":1753258325} from r1_node_3
MONOCEROS2025/07/23 08:12:05 monoceros.go:788: try lock
2025/07/23 08:12:05 metrics get
2025/07/23 08:12:05 # HELP app_request_processing_time_seconds Average request processing time
# TYPE app_request_processing_time_seconds gauge
app_request_processing_time_seconds 0.256

# HELP app_memory_usage_bytes Current memory usage in bytes
# TYPE app_memory_usage_bytes gauge
app_memory_usage_bytes 512

# HELP app_cpu_load_ratio CPU load (0-1)
# TYPE app_cpu_load_ratio gauge
app_cpu_load_ratio 0.13

# HELP app_active_sessions Current active user sessions
# TYPE app_active_sessions gauge
app_active_sessions 42

# HELP app_queue_depth_pending_jobs Jobs waiting in queue
# TYPE app_queue_depth_pending_jobs gauge
app_queue_depth_pending_jobs 7

# HELP app_cache_hit_ratio Cache hit ratio
# TYPE app_cache_hit_ratio gauge
app_cache_hit_ratio 0.82

# HELP app_current_goroutines Goroutine count
# TYPE app_current_goroutines gauge
app_current_goroutines 33

# HELP app_last_backup_timestamp_seconds Unix timestamp of last successful backup
# TYPE app_last_backup_timestamp_seconds gauge
app_last_backup_timestamp_seconds 1.700000e+09

# HELP app_http_requests_total Total HTTP requests processed
# TYPE app_http_requests_total counter
app_http_requests_total 12890

# HELP app_errors_total Total errors encountered
# TYPE app_errors_total counter
app_errors_total 17

# EOF

MONOCEROS2025/07/23 08:12:05 metric.go:83: metrics received
MONOCEROS2025/07/23 08:12:05 metric.go:84: # HELP app_request_processing_time_seconds Average request processing time
# TYPE app_request_processing_time_seconds gauge
app_request_processing_time_seconds 0.256

# HELP app_memory_usage_bytes Current memory usage in bytes
# TYPE app_memory_usage_bytes gauge
app_memory_usage_bytes 512

# HELP app_cpu_load_ratio CPU load (0-1)
# TYPE app_cpu_load_ratio gauge
app_cpu_load_ratio 0.13

# HELP app_active_sessions Current active user sessions
# TYPE app_active_sessions gauge
app_active_sessions 42

# HELP app_queue_depth_pending_jobs Jobs waiting in queue
# TYPE app_queue_depth_pending_jobs gauge
app_queue_depth_pending_jobs 7

# HELP app_cache_hit_ratio Cache hit ratio
# TYPE app_cache_hit_ratio gauge
app_cache_hit_ratio 0.82

# HELP app_current_goroutines Goroutine count
# TYPE app_current_goroutines gauge
app_current_goroutines 33

# HELP app_last_backup_timestamp_seconds Unix timestamp of last successful backup
# TYPE app_last_backup_timestamp_seconds gauge
app_last_backup_timestamp_seconds 1.700000e+09

# HELP app_http_requests_total Total HTTP requests processed
# TYPE app_http_requests_total counter
app_http_requests_total 12890

# HELP app_errors_total Total errors encountered
# TYPE app_errors_total counter
app_errors_total 17

# EOF

MONOCEROS2025/07/23 08:12:05 aggregation.go:228: get node metrics
MONOCEROS2025/07/23 08:12:05 aggregation.go:230: { {app_memory_usage_bytes map[]} 3 {total_app_memory_usage_bytes map[func:sum]}}
MONOCEROS2025/07/23 08:12:05 aggregation.go:232: [512]
MONOCEROS2025/07/23 08:12:05 aggregation.go:235: &{{total_app_memory_usage_bytes map[func:sum]} {512}}
MONOCEROS2025/07/23 08:12:05 aggregation.go:240: local [{{total_app_memory_usage_bytes map[func:sum]} {512}}]
MONOCEROS2025/07/23 08:12:05 aggregation.go:230: { {app_memory_usage_bytes map[]} 4 {avg_app_memory_usage_bytes map[func:avg]}}
MONOCEROS2025/07/23 08:12:05 aggregation.go:232: [512]
MONOCEROS2025/07/23 08:12:05 aggregation.go:235: &{{avg_app_memory_usage_bytes map[func:avg]} {512 1}}
MONOCEROS2025/07/23 08:12:05 aggregation.go:240: local [{{total_app_memory_usage_bytes map[func:sum]} {512}} {{avg_app_memory_usage_bytes map[func:avg]} {512 1}}]
MONOCEROS2025/07/23 08:12:05 aggregation.go:230: { {avg_app_memory_usage_bytes map[]} 4 {avg_app_memory_usage_bytes map[global:y]}}
MONOCEROS2025/07/23 08:12:05 aggregation.go:232: []
MONOCEROS2025/07/23 08:12:05 aggregation.go:235: <nil>
MONOCEROS2025/07/23 08:12:05 aggregation.go:230: { {total_app_memory_usage_bytes map[]} 3 {total_app_memory_usage_bytes map[global:y]}}
MONOCEROS2025/07/23 08:12:05 aggregation.go:232: []
MONOCEROS2025/07/23 08:12:05 aggregation.go:235: <nil>
REGION PT2025/07/23 08:12:05 plumtree.go:226: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:673: children to send req [{r1_node_2 r1_node_2:6001} {r1_node_4 r1_node_4:6001} {r1_node_5 r1_node_5:6001} {r1_node_7 r1_node_7:6001}]
REGION PT2025/07/23 08:12:05 plumtree.go:205: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:208: r1_node_1 - Get parent [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}]
MONOCEROS2025/07/23 08:12:05 monoceros.go:674: has parent true
REGION PT2025/07/23 08:12:05 msg_handlers.go:112: try lock
REGION PT2025/07/23 08:12:05 tree.go:88: r1_node_1 - Eager push - sending
REGION PT2025/07/23 08:12:05 tree.go:91: r1_node_1 - peer {{r1_node_2 r1_node_2:6001} 0xc0000a6700}
REGION PT2025/07/23 08:12:05 tree.go:95: r1_node_1 - Sending gossip msg to peer: r1_node_2
REGION PT2025/07/23 08:12:05 tree.go:91: r1_node_1 - peer {{r1_node_3 r1_node_3:6001} 0xc00013e000}
REGION PT2025/07/23 08:12:05 tree.go:91: r1_node_1 - peer {{r1_node_4 r1_node_4:6001} 0xc000242340}
REGION PT2025/07/23 08:12:05 tree.go:95: r1_node_1 - Sending gossip msg to peer: r1_node_4
REGION HV2025/07/23 08:12:05 hyparview.go:214: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:277: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:791: tree constructed in RN, should join RRN {RN_r1_node_3 r1_node_3}
MONOCEROS2025/07/23 08:12:05 monoceros.go:793: should not
REGION PT2025/07/23 08:12:05 tree.go:91: r1_node_1 - peer {{r1_node_5 r1_node_5:6001} 0xc0002423c0}
REGION PT2025/07/23 08:12:05 tree.go:95: r1_node_1 - Sending gossip msg to peer: r1_node_5
REGION PT2025/07/23 08:12:05 tree.go:91: r1_node_1 - peer {{r1_node_7 r1_node_7:6001} 0xc000298240}
REGION PT2025/07/23 08:12:05 tree.go:95: r1_node_1 - Sending gossip msg to peer: r1_node_7
REGION PT2025/07/23 08:12:05 tree.go:114: r1_node_1 - Lazy push - sending
REGION PT2025/07/23 08:12:05 plumtree.go:279: r1_node_1 - Custom message handler invoked
REGION PT2025/07/23 08:12:05 plumtree.go:281: r1_node_1 - sender &{192.168.240.3:43200 0xc000068200 0xc000094ee0 0xc000094f50 0x683d00 0xc0000a34a0}
REGION PT2025/07/23 08:12:05 plumtree.go:295: r1_node_1 - received from r1_node_2
REGION PT2025/07/23 08:12:05 plumtree.go:304: r1_node_1 - Received message in subscription handler [0 123 34 77 101 116 97 100 97 116 97 34 58 123 34 73 100 34 58 34 82 78 95 114 49 95 110 111 100 101 95 51 34 44 34 83 99 111 114 101 34 58 34 114 49 95 110 111 100 101 95 51 34 125 44 34 77 115 103 84 121 112 101 34 58 34 65 95 82 69 81 34 44 34 77 115 103 34 58 34 101 121 74 85 97 87 49 108 99 51 82 104 98 88 65 105 79 106 69 51 78 84 77 121 78 84 103 122 77 106 86 57 34 44 34 77 115 103 73 100 34 58 34 112 82 67 107 90 97 81 101 68 107 81 61 34 44 34 82 111 117 110 100 34 58 50 125]
REGION PT2025/07/23 08:12:05 plumtree.go:305: try lock
REGION PT2025/07/23 08:12:05 msg_handlers.go:97: r1_node_1 - Processing gossip message
REGION PT2025/07/23 08:12:05 msg_handlers.go:122: r1_node_1 - Removing peer r1_node_2 from eager push peers due to duplicate message
REGION HV2025/07/23 08:12:05 hyparview.go:214: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:277: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:279: r1_node_1 - Custom message handler invoked
GLOBAL HV2025/07/23 08:12:05 hyparview.go:214: try lock
2025/07/23 08:12:05 received gossip msg [85 153 128 104 0 0 0 0 1 123 34 74 111 105 110 101 100 34 58 102 97 108 115 101 44 34 78 111 100 101 73 110 102 111 34 58 123 34 73 68 34 58 34 114 49 95 110 111 100 101 95 51 34 44 34 76 105 115 116 101 110 65 100 100 114 101 115 115 34 58 34 114 49 95 110 111 100 101 95 51 58 56 48 48 49 34 125 44 34 82 101 103 105 111 110 34 58 34 114 49 34 125]
2025/07/23 08:12:05 Received: [1 123 34 74 111 105 110 101 100 34 58 102 97 108 115 101 44 34 78 111 100 101 73 110 102 111 34 58 123 34 73 68 34 58 34 114 49 95 110 111 100 101 95 51 34 44 34 76 105 115 116 101 110 65 100 100 114 101 115 115 34 58 34 114 49 95 110 111 100 101 95 51 58 56 48 48 49 34 125 44 34 82 101 103 105 111 110 34 58 34 114 49 34 125]
MONOCEROS2025/07/23 08:12:05 monoceros.go:424: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:427: received global network msg [1 123 34 74 111 105 110 101 100 34 58 102 97 108 115 101 44 34 78 111 100 101 73 110 102 111 34 58 123 34 73 68 34 58 34 114 49 95 110 111 100 101 95 51 34 44 34 76 105 115 116 101 110 65 100 100 114 101 115 115 34 58 34 114 49 95 110 111 100 101 95 51 58 56 48 48 49 34 125 44 34 82 101 103 105 111 110 34 58 34 114 49 34 125]
MONOCEROS2025/07/23 08:12:05 monoceros.go:430: received regional root update
MONOCEROS2025/07/23 08:12:05 monoceros.go:452: map[]
MONOCEROS2025/07/23 08:12:05 monoceros.go:453: map[]
GLOBAL HV2025/07/23 08:12:05 hyparview.go:156: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:281: r1_node_1 - sender &{192.168.240.5:6001 0xc000236070 0xc000240540 0xc0002405b0 0x683d00 0xc0000a34a0}
REGION PT2025/07/23 08:12:05 plumtree.go:295: r1_node_1 - received from r1_node_4
REGION HV2025/07/23 08:12:05 hyparview.go:214: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:277: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:279: r1_node_1 - Custom message handler invoked
REGION PT2025/07/23 08:12:05 plumtree.go:281: r1_node_1 - sender &{192.168.240.6:58506 0xc000236078 0xc000240620 0xc000240690 0x683d00 0xc0000a34a0}
REGION PT2025/07/23 08:12:05 plumtree.go:295: r1_node_1 - received from r1_node_5
REGION PT2025/07/23 08:12:05 plumtree.go:304: r1_node_1 - Received message in subscription handler [2 123 34 77 101 116 97 100 97 116 97 34 58 123 34 73 100 34 58 34 82 78 95 114 49 95 110 111 100 101 95 51 34 44 34 83 99 111 114 101 34 58 34 114 49 95 110 111 100 101 95 51 34 125 125]
REGION PT2025/07/23 08:12:05 plumtree.go:305: try lock
REGION PT2025/07/23 08:12:05 msg_handlers.go:142: r1_node_1 - Processing prune message from peer: r1_node_4
REGION PT2025/07/23 08:12:05 msg_handlers.go:143: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}] lazy push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700}]
REGION PT2025/07/23 08:12:05 msg_handlers.go:145: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}] lazy push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_4 r1_node_4:6001} 0xc000242340}]
REGION PT2025/07/23 08:12:05 plumtree.go:304: r1_node_1 - Received message in subscription handler [1 123 34 77 101 116 97 100 97 116 97 34 58 123 34 73 100 34 58 34 82 78 95 114 49 95 110 111 100 101 95 51 34 44 34 83 99 111 114 101 34 58 34 114 49 95 110 111 100 101 95 51 34 125 44 34 77 115 103 84 121 112 101 34 58 34 65 66 79 82 84 95 82 69 83 80 34 44 34 77 115 103 34 58 34 101 121 74 85 97 87 49 108 99 51 82 104 98 88 65 105 79 106 69 51 78 84 77 121 78 84 103 122 77 106 86 57 34 44 34 77 115 103 73 100 34 58 34 98 89 67 110 74 43 56 84 48 116 52 61 34 44 34 82 111 117 110 100 34 58 48 125]
REGION PT2025/07/23 08:12:05 plumtree.go:305: try lock
REGION PT2025/07/23 08:12:05 msg_handlers.go:136: r1_node_1 - Processing direct message
MONOCEROS2025/07/23 08:12:05 monoceros.go:394: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:397: received direct msg {RN_r1_node_3 r1_node_3} ABORT_RESP {"Timestamp":1753258325} from r1_node_5
MONOCEROS2025/07/23 08:12:05 monoceros.go:692: complete aggregation req &{RN 0xc000034200 1753258325 0 0xc000094bd0 <nil> 0 [] true 0x78c980 map[level:region regionID:r1]} {RN_r1_node_3 r1_node_3}
MONOCEROS2025/07/23 08:12:05 monoceros.go:693: <nil>
MONOCEROS2025/07/23 08:12:05 monoceros.go:694: RN_r1_node_3
REGION PT2025/07/23 08:12:05 plumtree.go:205: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:208: r1_node_1 - Get parent [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}]
MONOCEROS2025/07/23 08:12:05 monoceros.go:696: has parent
REGION PT2025/07/23 08:12:05 plumtree.go:165: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:168: r1_node_1 - send to parent [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}]
REGION PT2025/07/23 08:12:05 plumtree.go:183: r1_node_1 - Sending message
REGION HV2025/07/23 08:12:05 hyparview.go:214: try lock
REGION PT2025/07/23 08:12:05 tree.go:76: r1_node_1 - Sending direct message
REGION PT2025/07/23 08:12:05 tree.go:82: r1_node_1 - Message sent successfully
MONOCEROS2025/07/23 08:12:05 monoceros.go:719: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:277: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:279: r1_node_1 - Custom message handler invoked
REGION PT2025/07/23 08:12:05 plumtree.go:281: r1_node_1 - sender &{192.168.240.3:43200 0xc000068200 0xc000094ee0 0xc000094f50 0x683d00 0xc0000a34a0}
REGION PT2025/07/23 08:12:05 plumtree.go:295: r1_node_1 - received from r1_node_2
REGION PT2025/07/23 08:12:05 plumtree.go:304: r1_node_1 - Received message in subscription handler [2 123 34 77 101 116 97 100 97 116 97 34 58 123 34 73 100 34 58 34 82 78 95 114 49 95 110 111 100 101 95 51 34 44 34 83 99 111 114 101 34 58 34 114 49 95 110 111 100 101 95 51 34 125 125]
REGION PT2025/07/23 08:12:05 plumtree.go:305: try lock
REGION PT2025/07/23 08:12:05 msg_handlers.go:142: r1_node_1 - Processing prune message from peer: r1_node_2
REGION PT2025/07/23 08:12:05 msg_handlers.go:143: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}] lazy push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_4 r1_node_4:6001} 0xc000242340}]
REGION PT2025/07/23 08:12:05 msg_handlers.go:145: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}] lazy push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_4 r1_node_4:6001} 0xc000242340}]
GLOBAL HV2025/07/23 08:12:05 hyparview.go:214: try lock
2025/07/23 08:12:05 received gossip msg [85 153 128 104 0 0 0 0 1 123 34 74 111 105 110 101 100 34 58 102 97 108 115 101 44 34 78 111 100 101 73 110 102 111 34 58 123 34 73 68 34 58 34 114 49 95 110 111 100 101 95 51 34 44 34 76 105 115 116 101 110 65 100 100 114 101 115 115 34 58 34 114 49 95 110 111 100 101 95 51 58 56 48 48 49 34 125 44 34 82 101 103 105 111 110 34 58 34 114 49 34 125]
2025/07/23 08:12:05 msg already seen: [85 153 128 104 0 0 0 0 1 123 34 74 111 105 110 101 100 34 58 102 97 108 115 101 44 34 78 111 100 101 73 110 102 111 34 58 123 34 73 68 34 58 34 114 49 95 110 111 100 101 95 51 34 44 34 76 105 115 116 101 110 65 100 100 114 101 115 115 34 58 34 114 49 95 110 111 100 101 95 51 58 56 48 48 49 34 125 44 34 82 101 103 105 111 110 34 58 34 114 49 34 125]
GLOBAL HV2025/07/23 08:12:05 hyparview.go:214: try lock
2025/07/23 08:12:05 received gossip msg [85 153 128 104 0 0 0 0 1 123 34 74 111 105 110 101 100 34 58 102 97 108 115 101 44 34 78 111 100 101 73 110 102 111 34 58 123 34 73 68 34 58 34 114 49 95 110 111 100 101 95 51 34 44 34 76 105 115 116 101 110 65 100 100 114 101 115 115 34 58 34 114 49 95 110 111 100 101 95 51 58 56 48 48 49 34 125 44 34 82 101 103 105 111 110 34 58 34 114 49 34 125]
2025/07/23 08:12:05 msg already seen: [85 153 128 104 0 0 0 0 1 123 34 74 111 105 110 101 100 34 58 102 97 108 115 101 44 34 78 111 100 101 73 110 102 111 34 58 123 34 73 68 34 58 34 114 49 95 110 111 100 101 95 51 34 44 34 76 105 115 116 101 110 65 100 100 114 101 115 115 34 58 34 114 49 95 110 111 100 101 95 51 58 56 48 48 49 34 125 44 34 82 101 103 105 111 110 34 58 34 114 49 34 125]
REGION HV2025/07/23 08:12:05 hyparview.go:214: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:277: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:279: r1_node_1 - Custom message handler invoked
REGION PT2025/07/23 08:12:05 plumtree.go:281: r1_node_1 - sender &{192.168.240.8:6001 0xc0002a2020 0xc0002ca2a0 0xc0002ca310 0x683d00 0xc0000a34a0}
REGION PT2025/07/23 08:12:05 plumtree.go:295: r1_node_1 - received from r1_node_7
REGION PT2025/07/23 08:12:05 plumtree.go:304: r1_node_1 - Received message in subscription handler [1 123 34 77 101 116 97 100 97 116 97 34 58 123 34 73 100 34 58 34 82 78 95 114 49 95 110 111 100 101 95 51 34 44 34 83 99 111 114 101 34 58 34 114 49 95 110 111 100 101 95 51 34 125 44 34 77 115 103 84 121 112 101 34 58 34 65 66 79 82 84 95 82 69 83 80 34 44 34 77 115 103 34 58 34 101 121 74 85 97 87 49 108 99 51 82 104 98 88 65 105 79 106 69 51 78 84 77 121 78 84 103 122 77 106 86 57 34 44 34 77 115 103 73 100 34 58 34 105 111 97 51 43 122 79 72 87 81 103 61 34 44 34 82 111 117 110 100 34 58 48 125]
REGION PT2025/07/23 08:12:05 plumtree.go:305: try lock
REGION PT2025/07/23 08:12:05 msg_handlers.go:136: r1_node_1 - Processing direct message
MONOCEROS2025/07/23 08:12:05 monoceros.go:394: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:397: received direct msg {RN_r1_node_3 r1_node_3} ABORT_RESP {"Timestamp":1753258325} from r1_node_7
MONOCEROS2025/07/23 08:12:05 monoceros.go:615: could not find active request for response {1753258325} active requests []
MONOCEROS2025/07/23 08:12:05 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:05 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:05 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}]
MONOCEROS2025/07/23 08:12:05 monoceros.go:266: peers num 5 now time 1753258325 expected aggregation time 1.7532583302e+09
MONOCEROS2025/07/23 08:12:05 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:05 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:05 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}]
MONOCEROS2025/07/23 08:12:05 monoceros.go:266: peers num 5 now time 1753258325 expected aggregation time 1.7532583302e+09
MONOCEROS2025/07/23 08:12:05 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:05 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:05 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:05 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:05 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}]
MONOCEROS2025/07/23 08:12:05 monoceros.go:266: peers num 5 now time 1753258325 expected aggregation time 1.7532583302e+09
MONOCEROS2025/07/23 08:12:06 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:06 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:06 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:06 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:06 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:06 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}]
MONOCEROS2025/07/23 08:12:06 monoceros.go:266: peers num 5 now time 1753258326 expected aggregation time 1.7532583302e+09
MONOCEROS2025/07/23 08:12:06 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:06 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:06 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:06 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:06 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:06 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}]
MONOCEROS2025/07/23 08:12:06 monoceros.go:266: peers num 5 now time 1753258326 expected aggregation time 1.7532583302e+09
MONOCEROS2025/07/23 08:12:06 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:06 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:06 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:06 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:06 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:06 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}]
MONOCEROS2025/07/23 08:12:06 monoceros.go:266: peers num 5 now time 1753258326 expected aggregation time 1.7532583302e+09
MONOCEROS2025/07/23 08:12:06 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:06 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:06 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:06 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:06 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:06 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}]
MONOCEROS2025/07/23 08:12:06 monoceros.go:266: peers num 5 now time 1753258326 expected aggregation time 1.7532583302e+09
MONOCEROS2025/07/23 08:12:06 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:06 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:06 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:06 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:06 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}]
MONOCEROS2025/07/23 08:12:06 monoceros.go:266: peers num 5 now time 1753258326 expected aggregation time 1.7532583302e+09
MONOCEROS2025/07/23 08:12:06 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:06 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:06 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:06 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:06 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:06 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:06 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}]
MONOCEROS2025/07/23 08:12:06 monoceros.go:266: peers num 5 now time 1753258326 expected aggregation time 1.7532583302e+09
MONOCEROS2025/07/23 08:12:06 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:06 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:06 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:06 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:06 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:06 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}]
MONOCEROS2025/07/23 08:12:06 monoceros.go:266: peers num 5 now time 1753258326 expected aggregation time 1.7532583302e+09
MONOCEROS2025/07/23 08:12:06 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:06 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:06 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:06 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:06 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:06 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}]
MONOCEROS2025/07/23 08:12:06 monoceros.go:266: peers num 5 now time 1753258326 expected aggregation time 1.7532583302e+09
GLOBAL HV2025/07/23 08:12:06 hyparview.go:214: try lock
GLOBAL HV2025/07/23 08:12:06 msg_handlers.go:89: try lock
GLOBAL HV2025/07/23 08:12:06 msg_handlers.go:100: received ForwardJoin message self r1_node_1 from r1_node_8 TTL 0
REGION HV2025/07/23 08:12:06 hyparview.go:214: try lock
REGION HV2025/07/23 08:12:06 msg_handlers.go:89: try lock
REGION HV2025/07/23 08:12:06 msg_handlers.go:100: received ForwardJoin message self r1_node_1 from r1_node_8 TTL 3
REGION HV2025/07/23 08:12:06 msg_handlers.go:143: added peer to passive view via forward join peerID r1_node_8
GLOBAL HV2025/07/23 08:12:06 msg_handlers.go:138: added peer to active view via forward join peerID r1_node_8 address r1_node_8:7001
MONOCEROS2025/07/23 08:12:06 monoceros.go:775: try lock
MONOCEROS2025/07/23 08:12:06 monoceros.go:778: global network peer up
MONOCEROS2025/07/23 08:12:06 monoceros.go:780: already synced, no need to send sync req
2025/07/23 08:12:06 nothing to send to peer
REGION HV2025/07/23 08:12:06 conn_tcp.go:112: tcp read error: read tcp 192.168.240.2:33186->192.168.240.5:6001: read: connection reset by peer
REGION HV2025/07/23 08:12:06 hyparview.go:174: r1_node_1 - conn 192.168.240.5:6001 down
REGION HV2025/07/23 08:12:06 hyparview.go:175: try lock
REGION HV2025/07/23 08:12:06 hyparview.go:178: lock acquired
REGION HV2025/07/23 08:12:06 hyparview.go:180: r1_node_1 - peer r1_node_4 down
REGION HV2025/07/23 08:12:06 hyparview.go:255: r1_node_1 attempting to replace failed peer
REGION PT2025/07/23 08:12:06 plumtree.go:345: r1_node_1 - Processing onPeerDown peer: r1_node_4
REGION PT2025/07/23 08:12:06 plumtree.go:346: try lock
REGION PT2025/07/23 08:12:06 tree.go:209: r1_node_1 - Processing onPeerDown peer: r1_node_4
REGION PT2025/07/23 08:12:06 tree.go:210: r1_node_1 - eager push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}] lazy push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340}]
REGION PT2025/07/23 08:12:06 tree.go:217: r1_node_1 - eager push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}] lazy push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000}]
REGION PT2025/07/23 08:12:06 tree.go:209: r1_node_1 - Processing onPeerDown peer: r1_node_4
REGION PT2025/07/23 08:12:06 tree.go:210: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}] lazy push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_4 r1_node_4:6001} 0xc000242340}]
REGION PT2025/07/23 08:12:06 tree.go:217: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}] lazy push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700}]
REGION PT2025/07/23 08:12:06 tree.go:209: r1_node_1 - Processing onPeerDown peer: r1_node_4
REGION PT2025/07/23 08:12:06 tree.go:210: r1_node_1 - eager push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_4 r1_node_4:6001} 0xc000242340} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}] lazy push peers []
REGION PT2025/07/23 08:12:06 tree.go:217: r1_node_1 - eager push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}] lazy push peers []
REGION HV2025/07/23 08:12:06 hyparview.go:288: r1_node_1 sent NEIGHBOR message to r1_node_8
REGION HV2025/07/23 08:12:06 hyparview.go:214: try lock
REGION HV2025/07/23 08:12:06 msg_handlers.go:256: try lock
REGION HV2025/07/23 08:12:06 msg_handlers.go:267: received NeighborReply self r1_node_1 from r1_node_8 accepted true attempts 1
REGION HV2025/07/23 08:12:06 msg_handlers.go:321: added peer to active view from neighbor reply peerID r1_node_8 address r1_node_8:6001
REGION PT2025/07/23 08:12:06 plumtree.go:325: r1_node_1 - Processing onPeerUp peer: r1_node_8
REGION PT2025/07/23 08:12:06 plumtree.go:326: try lock
REGION PT2025/07/23 08:12:06 plumtree.go:334: r1_node_1 - Added peer r1_node_8 to peers
REGION PT2025/07/23 08:12:06 tree.go:196: r1_node_1 - Processing onPeerUp peer: r1_node_8
REGION PT2025/07/23 08:12:06 tree.go:197: r1_node_1 - eager push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}] lazy push peers []
REGION PT2025/07/23 08:12:06 tree.go:202: r1_node_1 - Added peer r1_node_8 to eager push peers
REGION PT2025/07/23 08:12:06 tree.go:204: r1_node_1 - eager push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940}] lazy push peers []
REGION PT2025/07/23 08:12:06 tree.go:196: r1_node_1 - Processing onPeerUp peer: r1_node_8
REGION PT2025/07/23 08:12:06 tree.go:197: r1_node_1 - eager push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}] lazy push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000}]
REGION PT2025/07/23 08:12:06 tree.go:202: r1_node_1 - Added peer r1_node_8 to eager push peers
REGION PT2025/07/23 08:12:06 tree.go:204: r1_node_1 - eager push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940}] lazy push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000}]
REGION PT2025/07/23 08:12:06 tree.go:196: r1_node_1 - Processing onPeerUp peer: r1_node_8
REGION PT2025/07/23 08:12:06 tree.go:197: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240}] lazy push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700}]
REGION PT2025/07/23 08:12:06 tree.go:202: r1_node_1 - Added peer r1_node_8 to eager push peers
REGION PT2025/07/23 08:12:06 tree.go:204: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940}] lazy push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700}]
MONOCEROS2025/07/23 08:12:06 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:06 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:06 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:06 plumtree.go:219: r1_node_1 - Get peers num
MONOCEROS2025/07/23 08:12:06 monoceros.go:250: try lock
REGION PT2025/07/23 08:12:06 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940}]
MONOCEROS2025/07/23 08:12:06 monoceros.go:266: peers num 5 now time 1753258326 expected aggregation time 1.7532583302e+09
REGION HV2025/07/23 08:12:06 hyparview.go:214: try lock
REGION PT2025/07/23 08:12:06 plumtree.go:277: try lock
REGION PT2025/07/23 08:12:06 plumtree.go:279: r1_node_1 - Custom message handler invoked
REGION PT2025/07/23 08:12:06 plumtree.go:281: r1_node_1 - sender &{192.168.240.4:6001 0xc000138000 0xc00013c000 0xc00013c070 0x683d00 0xc0000a34a0}
REGION PT2025/07/23 08:12:06 plumtree.go:295: r1_node_1 - received from r1_node_3
REGION PT2025/07/23 08:12:06 plumtree.go:304: r1_node_1 - Received message in subscription handler [0 123 34 77 101 116 97 100 97 116 97 34 58 123 34 73 100 34 58 34 82 78 95 114 49 95 110 111 100 101 95 52 34 44 34 83 99 111 114 101 34 58 34 114 49 95 110 111 100 101 95 52 34 125 44 34 77 115 103 84 121 112 101 34 58 34 65 95 82 69 81 34 44 34 77 115 103 34 58 34 101 121 74 85 97 87 49 108 99 51 82 104 98 88 65 105 79 106 69 51 78 84 77 121 78 84 103 122 77 106 90 57 34 44 34 77 115 103 73 100 34 58 34 89 100 69 83 86 121 69 79 49 48 107 61 34 44 34 82 111 117 110 100 34 58 50 125]
REGION PT2025/07/23 08:12:06 plumtree.go:305: try lock
REGION PT2025/07/23 08:12:06 msg_handlers.go:21: r1_node_1 - tree with id=RN_r1_node_4 not found
REGION PT2025/07/23 08:12:06 msg_handlers.go:23: r1_node_1 - tree created RN_r1_node_4
REGION PT2025/07/23 08:12:06 msg_handlers.go:97: r1_node_1 - Processing gossip message
REGION PT2025/07/23 08:12:06 msg_handlers.go:101: r1_node_1 - message [97 209 18 87 33 14 215 73] received for the first time add sender to eager push peers {r1_node_3 r1_node_3:6001}
MONOCEROS2025/07/23 08:12:06 monoceros.go:361: try lock
MONOCEROS2025/07/23 08:12:06 monoceros.go:364: received aggregation msg {RN_r1_node_4 r1_node_4} A_REQ {"Timestamp":1753258326} from r1_node_3
REGION HV2025/07/23 08:12:06 hyparview.go:214: try lock
REGION PT2025/07/23 08:12:06 plumtree.go:277: try lock
REGION PT2025/07/23 08:12:06 plumtree.go:279: r1_node_1 - Custom message handler invoked
REGION PT2025/07/23 08:12:06 plumtree.go:281: r1_node_1 - sender &{192.168.240.3:43200 0xc000068200 0xc000094ee0 0xc000094f50 0x683d00 0xc0000a34a0}
REGION PT2025/07/23 08:12:06 plumtree.go:295: r1_node_1 - received from r1_node_2
MONOCEROS2025/07/23 08:12:06 monoceros.go:788: try lock
2025/07/23 08:12:06 metrics get
2025/07/23 08:12:06 # HELP app_request_processing_time_seconds Average request processing time
# TYPE app_request_processing_time_seconds gauge
app_request_processing_time_seconds 0.256

# HELP app_memory_usage_bytes Current memory usage in bytes
# TYPE app_memory_usage_bytes gauge
app_memory_usage_bytes 512

# HELP app_cpu_load_ratio CPU load (0-1)
# TYPE app_cpu_load_ratio gauge
app_cpu_load_ratio 0.13

# HELP app_active_sessions Current active user sessions
# TYPE app_active_sessions gauge
app_active_sessions 42

# HELP app_queue_depth_pending_jobs Jobs waiting in queue
# TYPE app_queue_depth_pending_jobs gauge
app_queue_depth_pending_jobs 7

# HELP app_cache_hit_ratio Cache hit ratio
# TYPE app_cache_hit_ratio gauge
app_cache_hit_ratio 0.82

# HELP app_current_goroutines Goroutine count
# TYPE app_current_goroutines gauge
app_current_goroutines 33

# HELP app_last_backup_timestamp_seconds Unix timestamp of last successful backup
# TYPE app_last_backup_timestamp_seconds gauge
app_last_backup_timestamp_seconds 1.700000e+09

# HELP app_http_requests_total Total HTTP requests processed
# TYPE app_http_requests_total counter
app_http_requests_total 12890

# HELP app_errors_total Total errors encountered
# TYPE app_errors_total counter
app_errors_total 17

# EOF

GLOBAL HV2025/07/23 08:12:06 hyparview.go:214: try lock
2025/07/23 08:12:06 received gossip msg [86 153 128 104 0 0 0 0 1 123 34 74 111 105 110 101 100 34 58 102 97 108 115 101 44 34 78 111 100 101 73 110 102 111 34 58 123 34 73 68 34 58 34 114 49 95 110 111 100 101 95 52 34 44 34 76 105 115 116 101 110 65 100 100 114 101 115 115 34 58 34 114 49 95 110 111 100 101 95 52 58 56 48 48 49 34 125 44 34 82 101 103 105 111 110 34 58 34 114 49 34 125]
2025/07/23 08:12:06 Received: [1 123 34 74 111 105 110 101 100 34 58 102 97 108 115 101 44 34 78 111 100 101 73 110 102 111 34 58 123 34 73 68 34 58 34 114 49 95 110 111 100 101 95 52 34 44 34 76 105 115 116 101 110 65 100 100 114 101 115 115 34 58 34 114 49 95 110 111 100 101 95 52 58 56 48 48 49 34 125 44 34 82 101 103 105 111 110 34 58 34 114 49 34 125]
MONOCEROS2025/07/23 08:12:06 monoceros.go:424: try lock
MONOCEROS2025/07/23 08:12:06 metric.go:83: metrics received
MONOCEROS2025/07/23 08:12:06 metric.go:84: # HELP app_request_processing_time_seconds Average request processing time
# TYPE app_request_processing_time_seconds gauge
app_request_processing_time_seconds 0.256

# HELP app_memory_usage_bytes Current memory usage in bytes
# TYPE app_memory_usage_bytes gauge
app_memory_usage_bytes 512

# HELP app_cpu_load_ratio CPU load (0-1)
# TYPE app_cpu_load_ratio gauge
app_cpu_load_ratio 0.13

# HELP app_active_sessions Current active user sessions
# TYPE app_active_sessions gauge
app_active_sessions 42

# HELP app_queue_depth_pending_jobs Jobs waiting in queue
# TYPE app_queue_depth_pending_jobs gauge
app_queue_depth_pending_jobs 7

# HELP app_cache_hit_ratio Cache hit ratio
# TYPE app_cache_hit_ratio gauge
app_cache_hit_ratio 0.82

# HELP app_current_goroutines Goroutine count
# TYPE app_current_goroutines gauge
app_current_goroutines 33

# HELP app_last_backup_timestamp_seconds Unix timestamp of last successful backup
# TYPE app_last_backup_timestamp_seconds gauge
app_last_backup_timestamp_seconds 1.700000e+09

# HELP app_http_requests_total Total HTTP requests processed
# TYPE app_http_requests_total counter
app_http_requests_total 12890

# HELP app_errors_total Total errors encountered
# TYPE app_errors_total counter
app_errors_total 17

# EOF

MONOCEROS2025/07/23 08:12:06 aggregation.go:228: get node metrics
MONOCEROS2025/07/23 08:12:06 aggregation.go:230: { {app_memory_usage_bytes map[]} 3 {total_app_memory_usage_bytes map[func:sum]}}
MONOCEROS2025/07/23 08:12:06 aggregation.go:232: [512]
MONOCEROS2025/07/23 08:12:06 aggregation.go:235: &{{total_app_memory_usage_bytes map[func:sum]} {512}}
MONOCEROS2025/07/23 08:12:06 aggregation.go:240: local [{{total_app_memory_usage_bytes map[func:sum]} {512}}]
MONOCEROS2025/07/23 08:12:06 aggregation.go:230: { {app_memory_usage_bytes map[]} 4 {avg_app_memory_usage_bytes map[func:avg]}}
MONOCEROS2025/07/23 08:12:06 aggregation.go:232: [512]
MONOCEROS2025/07/23 08:12:06 aggregation.go:235: &{{avg_app_memory_usage_bytes map[func:avg]} {512 1}}
MONOCEROS2025/07/23 08:12:06 aggregation.go:240: local [{{total_app_memory_usage_bytes map[func:sum]} {512}} {{avg_app_memory_usage_bytes map[func:avg]} {512 1}}]
MONOCEROS2025/07/23 08:12:06 aggregation.go:230: { {avg_app_memory_usage_bytes map[]} 4 {avg_app_memory_usage_bytes map[global:y]}}
MONOCEROS2025/07/23 08:12:06 aggregation.go:232: []
MONOCEROS2025/07/23 08:12:06 aggregation.go:235: <nil>
MONOCEROS2025/07/23 08:12:06 aggregation.go:230: { {total_app_memory_usage_bytes map[]} 3 {total_app_memory_usage_bytes map[global:y]}}
MONOCEROS2025/07/23 08:12:06 aggregation.go:232: []
MONOCEROS2025/07/23 08:12:06 aggregation.go:235: <nil>
REGION PT2025/07/23 08:12:06 plumtree.go:226: try lock
MONOCEROS2025/07/23 08:12:06 monoceros.go:673: children to send req [{r1_node_2 r1_node_2:6001} {r1_node_5 r1_node_5:6001} {r1_node_7 r1_node_7:6001} {r1_node_8 r1_node_8:6001}]
REGION PT2025/07/23 08:12:06 plumtree.go:205: try lock
REGION PT2025/07/23 08:12:06 plumtree.go:208: r1_node_1 - Get parent [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940}]
MONOCEROS2025/07/23 08:12:06 monoceros.go:674: has parent true
REGION PT2025/07/23 08:12:06 msg_handlers.go:112: try lock
REGION PT2025/07/23 08:12:06 tree.go:88: r1_node_1 - Eager push - sending
REGION PT2025/07/23 08:12:06 tree.go:91: r1_node_1 - peer {{r1_node_2 r1_node_2:6001} 0xc0000a6700}
REGION PT2025/07/23 08:12:06 tree.go:95: r1_node_1 - Sending gossip msg to peer: r1_node_2
REGION PT2025/07/23 08:12:06 tree.go:91: r1_node_1 - peer {{r1_node_3 r1_node_3:6001} 0xc00013e000}
REGION PT2025/07/23 08:12:06 tree.go:91: r1_node_1 - peer {{r1_node_5 r1_node_5:6001} 0xc0002423c0}
REGION PT2025/07/23 08:12:06 tree.go:95: r1_node_1 - Sending gossip msg to peer: r1_node_5
REGION PT2025/07/23 08:12:06 tree.go:91: r1_node_1 - peer {{r1_node_7 r1_node_7:6001} 0xc000298240}
REGION PT2025/07/23 08:12:06 tree.go:95: r1_node_1 - Sending gossip msg to peer: r1_node_7
REGION PT2025/07/23 08:12:06 tree.go:91: r1_node_1 - peer {{r1_node_8 r1_node_8:6001} 0xc00032b940}
REGION PT2025/07/23 08:12:06 tree.go:95: r1_node_1 - Sending gossip msg to peer: r1_node_8
REGION PT2025/07/23 08:12:06 tree.go:114: r1_node_1 - Lazy push - sending
REGION PT2025/07/23 08:12:06 plumtree.go:304: r1_node_1 - Received message in subscription handler [0 123 34 77 101 116 97 100 97 116 97 34 58 123 34 73 100 34 58 34 82 78 95 114 49 95 110 111 100 101 95 52 34 44 34 83 99 111 114 101 34 58 34 114 49 95 110 111 100 101 95 52 34 125 44 34 77 115 103 84 121 112 101 34 58 34 65 95 82 69 81 34 44 34 77 115 103 34 58 34 101 121 74 85 97 87 49 108 99 51 82 104 98 88 65 105 79 106 69 51 78 84 77 121 78 84 103 122 77 106 90 57 34 44 34 77 115 103 73 100 34 58 34 89 100 69 83 86 121 69 79 49 48 107 61 34 44 34 82 111 117 110 100 34 58 50 125]
REGION PT2025/07/23 08:12:06 plumtree.go:305: try lock
REGION PT2025/07/23 08:12:06 msg_handlers.go:97: r1_node_1 - Processing gossip message
REGION PT2025/07/23 08:12:06 msg_handlers.go:122: r1_node_1 - Removing peer r1_node_2 from eager push peers due to duplicate message
MONOCEROS2025/07/23 08:12:06 monoceros.go:791: tree constructed in RN, should join RRN {RN_r1_node_4 r1_node_4}
MONOCEROS2025/07/23 08:12:06 monoceros.go:793: should not
MONOCEROS2025/07/23 08:12:06 monoceros.go:427: received global network msg [1 123 34 74 111 105 110 101 100 34 58 102 97 108 115 101 44 34 78 111 100 101 73 110 102 111 34 58 123 34 73 68 34 58 34 114 49 95 110 111 100 101 95 52 34 44 34 76 105 115 116 101 110 65 100 100 114 101 115 115 34 58 34 114 49 95 110 111 100 101 95 52 58 56 48 48 49 34 125 44 34 82 101 103 105 111 110 34 58 34 114 49 34 125]
MONOCEROS2025/07/23 08:12:06 monoceros.go:430: received regional root update
REGION HV2025/07/23 08:12:06 hyparview.go:214: try lock
REGION PT2025/07/23 08:12:06 plumtree.go:277: try lock
REGION PT2025/07/23 08:12:06 plumtree.go:279: r1_node_1 - Custom message handler invoked
REGION PT2025/07/23 08:12:06 plumtree.go:281: r1_node_1 - sender &{192.168.240.3:43200 0xc000068200 0xc000094ee0 0xc000094f50 0x683d00 0xc0000a34a0}
REGION PT2025/07/23 08:12:06 plumtree.go:295: r1_node_1 - received from r1_node_2
REGION PT2025/07/23 08:12:06 plumtree.go:304: r1_node_1 - Received message in subscription handler [2 123 34 77 101 116 97 100 97 116 97 34 58 123 34 73 100 34 58 34 82 78 95 114 49 95 110 111 100 101 95 52 34 44 34 83 99 111 114 101 34 58 34 114 49 95 110 111 100 101 95 52 34 125 125]
REGION PT2025/07/23 08:12:06 plumtree.go:305: try lock
MONOCEROS2025/07/23 08:12:06 monoceros.go:452: map[]
REGION PT2025/07/23 08:12:06 msg_handlers.go:142: r1_node_1 - Processing prune message from peer: r1_node_2
REGION PT2025/07/23 08:12:06 msg_handlers.go:143: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940}] lazy push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700}]
REGION PT2025/07/23 08:12:06 msg_handlers.go:145: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940}] lazy push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700}]
REGION HV2025/07/23 08:12:06 hyparview.go:214: try lock
REGION PT2025/07/23 08:12:06 plumtree.go:277: try lock
REGION PT2025/07/23 08:12:06 plumtree.go:279: r1_node_1 - Custom message handler invoked
REGION PT2025/07/23 08:12:06 plumtree.go:281: r1_node_1 - sender &{192.168.240.8:6001 0xc0002a2020 0xc0002ca2a0 0xc0002ca310 0x683d00 0xc0000a34a0}
REGION PT2025/07/23 08:12:06 plumtree.go:295: r1_node_1 - received from r1_node_7
REGION HV2025/07/23 08:12:06 hyparview.go:214: try lock
REGION PT2025/07/23 08:12:06 plumtree.go:277: try lock
REGION PT2025/07/23 08:12:06 plumtree.go:279: r1_node_1 - Custom message handler invoked
REGION PT2025/07/23 08:12:06 plumtree.go:281: r1_node_1 - sender &{192.168.240.6:58506 0xc000236078 0xc000240620 0xc000240690 0x683d00 0xc0000a34a0}
REGION PT2025/07/23 08:12:06 plumtree.go:295: r1_node_1 - received from r1_node_5
MONOCEROS2025/07/23 08:12:06 monoceros.go:453: map[]
GLOBAL HV2025/07/23 08:12:06 hyparview.go:156: try lock
GLOBAL HV2025/07/23 08:12:06 hyparview.go:214: try lock
2025/07/23 08:12:06 received gossip msg [86 153 128 104 0 0 0 0 1 123 34 74 111 105 110 101 100 34 58 102 97 108 115 101 44 34 78 111 100 101 73 110 102 111 34 58 123 34 73 68 34 58 34 114 49 95 110 111 100 101 95 52 34 44 34 76 105 115 116 101 110 65 100 100 114 101 115 115 34 58 34 114 49 95 110 111 100 101 95 52 58 56 48 48 49 34 125 44 34 82 101 103 105 111 110 34 58 34 114 49 34 125]
REGION PT2025/07/23 08:12:06 plumtree.go:304: r1_node_1 - Received message in subscription handler [2 123 34 77 101 116 97 100 97 116 97 34 58 123 34 73 100 34 58 34 82 78 95 114 49 95 110 111 100 101 95 52 34 44 34 83 99 111 114 101 34 58 34 114 49 95 110 111 100 101 95 52 34 125 125]
2025/07/23 08:12:06 msg already seen: [86 153 128 104 0 0 0 0 1 123 34 74 111 105 110 101 100 34 58 102 97 108 115 101 44 34 78 111 100 101 73 110 102 111 34 58 123 34 73 68 34 58 34 114 49 95 110 111 100 101 95 52 34 44 34 76 105 115 116 101 110 65 100 100 114 101 115 115 34 58 34 114 49 95 110 111 100 101 95 52 58 56 48 48 49 34 125 44 34 82 101 103 105 111 110 34 58 34 114 49 34 125]
REGION PT2025/07/23 08:12:06 plumtree.go:305: try lock
GLOBAL HV2025/07/23 08:12:06 hyparview.go:214: try lock
2025/07/23 08:12:06 received gossip msg [86 153 128 104 0 0 0 0 1 123 34 74 111 105 110 101 100 34 58 102 97 108 115 101 44 34 78 111 100 101 73 110 102 111 34 58 123 34 73 68 34 58 34 114 49 95 110 111 100 101 95 52 34 44 34 76 105 115 116 101 110 65 100 100 114 101 115 115 34 58 34 114 49 95 110 111 100 101 95 52 58 56 48 48 49 34 125 44 34 82 101 103 105 111 110 34 58 34 114 49 34 125]
REGION PT2025/07/23 08:12:06 msg_handlers.go:142: r1_node_1 - Processing prune message from peer: r1_node_7
2025/07/23 08:12:06 msg already seen: [86 153 128 104 0 0 0 0 1 123 34 74 111 105 110 101 100 34 58 102 97 108 115 101 44 34 78 111 100 101 73 110 102 111 34 58 123 34 73 68 34 58 34 114 49 95 110 111 100 101 95 52 34 44 34 76 105 115 116 101 110 65 100 100 114 101 115 115 34 58 34 114 49 95 110 111 100 101 95 52 58 56 48 48 49 34 125 44 34 82 101 103 105 111 110 34 58 34 114 49 34 125]
GLOBAL HV2025/07/23 08:12:06 hyparview.go:214: try lock
REGION PT2025/07/23 08:12:06 msg_handlers.go:143: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940}] lazy push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700}]
2025/07/23 08:12:06 received gossip msg [86 153 128 104 0 0 0 0 1 123 34 74 111 105 110 101 100 34 58 102 97 108 115 101 44 34 78 111 100 101 73 110 102 111 34 58 123 34 73 68 34 58 34 114 49 95 110 111 100 101 95 52 34 44 34 76 105 115 116 101 110 65 100 100 114 101 115 115 34 58 34 114 49 95 110 111 100 101 95 52 58 56 48 48 49 34 125 44 34 82 101 103 105 111 110 34 58 34 114 49 34 125]
2025/07/23 08:12:06 msg already seen: [86 153 128 104 0 0 0 0 1 123 34 74 111 105 110 101 100 34 58 102 97 108 115 101 44 34 78 111 100 101 73 110 102 111 34 58 123 34 73 68 34 58 34 114 49 95 110 111 100 101 95 52 34 44 34 76 105 115 116 101 110 65 100 100 114 101 115 115 34 58 34 114 49 95 110 111 100 101 95 52 58 56 48 48 49 34 125 44 34 82 101 103 105 111 110 34 58 34 114 49 34 125]
REGION PT2025/07/23 08:12:06 msg_handlers.go:145: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_8 r1_node_8:6001} 0xc00032b940}] lazy push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_7 r1_node_7:6001} 0xc000298240}]
REGION PT2025/07/23 08:12:06 plumtree.go:304: r1_node_1 - Received message in subscription handler [2 123 34 77 101 116 97 100 97 116 97 34 58 123 34 73 100 34 58 34 82 78 95 114 49 95 110 111 100 101 95 52 34 44 34 83 99 111 114 101 34 58 34 114 49 95 110 111 100 101 95 52 34 125 125]
REGION PT2025/07/23 08:12:06 plumtree.go:305: try lock
REGION PT2025/07/23 08:12:06 msg_handlers.go:142: r1_node_1 - Processing prune message from peer: r1_node_5
REGION PT2025/07/23 08:12:06 msg_handlers.go:143: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_8 r1_node_8:6001} 0xc00032b940}] lazy push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_7 r1_node_7:6001} 0xc000298240}]
REGION PT2025/07/23 08:12:06 msg_handlers.go:145: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_8 r1_node_8:6001} 0xc00032b940}] lazy push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
REGION HV2025/07/23 08:12:06 hyparview.go:214: try lock
REGION PT2025/07/23 08:12:06 plumtree.go:277: try lock
REGION PT2025/07/23 08:12:06 plumtree.go:279: r1_node_1 - Custom message handler invoked
REGION PT2025/07/23 08:12:06 plumtree.go:281: r1_node_1 - sender &{192.168.240.9:6001 0xc00032e290 0xc000340700 0xc000340770 0x683d00 0xc0000a34a0}
REGION PT2025/07/23 08:12:06 plumtree.go:295: r1_node_1 - received from r1_node_8
REGION PT2025/07/23 08:12:06 plumtree.go:304: r1_node_1 - Received message in subscription handler [2 123 34 77 101 116 97 100 97 116 97 34 58 123 34 73 100 34 58 34 82 78 95 114 49 95 110 111 100 101 95 52 34 44 34 83 99 111 114 101 34 58 34 114 49 95 110 111 100 101 95 52 34 125 125]
REGION PT2025/07/23 08:12:06 plumtree.go:305: try lock
REGION PT2025/07/23 08:12:06 msg_handlers.go:142: r1_node_1 - Processing prune message from peer: r1_node_8
REGION PT2025/07/23 08:12:06 msg_handlers.go:143: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_8 r1_node_8:6001} 0xc00032b940}] lazy push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_5 r1_node_5:6001} 0xc0002423c0}]
REGION PT2025/07/23 08:12:06 msg_handlers.go:145: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000}] lazy push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_8 r1_node_8:6001} 0xc00032b940}]
MONOCEROS2025/07/23 08:12:06 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:06 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:06 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:06 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:06 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940}]
MONOCEROS2025/07/23 08:12:06 monoceros.go:266: peers num 5 now time 1753258326 expected aggregation time 1.7532583312e+09
MONOCEROS2025/07/23 08:12:06 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:07 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:07 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:07 plumtree.go:216: try lock
MONOCEROS2025/07/23 08:12:07 monoceros.go:250: try lock
REGION PT2025/07/23 08:12:07 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:07 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940}]
MONOCEROS2025/07/23 08:12:07 monoceros.go:266: peers num 5 now time 1753258327 expected aggregation time 1.7532583312e+09
MONOCEROS2025/07/23 08:12:07 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:07 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:07 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:07 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:07 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940}]
MONOCEROS2025/07/23 08:12:07 monoceros.go:266: peers num 5 now time 1753258327 expected aggregation time 1.7532583312e+09
MONOCEROS2025/07/23 08:12:07 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:07 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:07 monoceros.go:256: try promote
MONOCEROS2025/07/23 08:12:07 monoceros.go:250: try lock
REGION PT2025/07/23 08:12:07 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:07 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:07 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940}]
MONOCEROS2025/07/23 08:12:07 monoceros.go:266: peers num 5 now time 1753258327 expected aggregation time 1.7532583312e+09
MONOCEROS2025/07/23 08:12:07 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:07 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:07 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:07 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:07 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940}]
MONOCEROS2025/07/23 08:12:07 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:07 monoceros.go:266: peers num 5 now time 1753258327 expected aggregation time 1.7532583312e+09
MONOCEROS2025/07/23 08:12:07 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:07 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:07 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:07 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:07 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:07 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940}]
MONOCEROS2025/07/23 08:12:07 monoceros.go:266: peers num 5 now time 1753258327 expected aggregation time 1.7532583312e+09
MONOCEROS2025/07/23 08:12:07 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:07 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:07 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:07 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:07 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:07 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940}]
MONOCEROS2025/07/23 08:12:07 monoceros.go:266: peers num 5 now time 1753258327 expected aggregation time 1.7532583312e+09
MONOCEROS2025/07/23 08:12:07 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:07 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:07 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:07 plumtree.go:219: r1_node_1 - Get peers num
MONOCEROS2025/07/23 08:12:07 monoceros.go:250: try lock
REGION PT2025/07/23 08:12:07 plumtree.go:220: r1_node_1 - [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940}]
MONOCEROS2025/07/23 08:12:07 monoceros.go:266: peers num 5 now time 1753258327 expected aggregation time 1.7532583312e+09
GLOBAL HV2025/07/23 08:12:07 conn_tcp.go:112: tcp read error: read tcp 192.168.240.2:7001->192.168.240.3:41898: read: connection reset by peer
GLOBAL HV2025/07/23 08:12:07 hyparview.go:174: r1_node_1 - conn 192.168.240.3:41898 down
GLOBAL HV2025/07/23 08:12:07 hyparview.go:175: try lock
GLOBAL HV2025/07/23 08:12:07 hyparview.go:178: lock acquired
GLOBAL HV2025/07/23 08:12:07 hyparview.go:180: r1_node_1 - peer r1_node_2 down
GLOBAL HV2025/07/23 08:12:07 hyparview.go:255: r1_node_1 attempting to replace failed peer
REGION HV2025/07/23 08:12:07 conn_tcp.go:112: tcp read error: read tcp 192.168.240.2:6001->192.168.240.3:43200: read: connection reset by peer
REGION HV2025/07/23 08:12:07 hyparview.go:174: r1_node_1 - conn 192.168.240.3:43200 down
REGION HV2025/07/23 08:12:07 hyparview.go:175: try lock
REGION HV2025/07/23 08:12:07 hyparview.go:178: lock acquired
REGION HV2025/07/23 08:12:07 hyparview.go:180: r1_node_1 - peer r1_node_2 down
REGION HV2025/07/23 08:12:07 hyparview.go:255: r1_node_1 attempting to replace failed peer
REGION PT2025/07/23 08:12:07 plumtree.go:345: r1_node_1 - Processing onPeerDown peer: r1_node_2
REGION PT2025/07/23 08:12:07 plumtree.go:346: try lock
REGION PT2025/07/23 08:12:07 tree.go:209: r1_node_1 - Processing onPeerDown peer: r1_node_2
REGION PT2025/07/23 08:12:07 tree.go:210: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000}] lazy push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_8 r1_node_8:6001} 0xc00032b940}]
REGION PT2025/07/23 08:12:07 tree.go:217: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000}] lazy push peers [{{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_8 r1_node_8:6001} 0xc00032b940}]
REGION PT2025/07/23 08:12:07 tree.go:209: r1_node_1 - Processing onPeerDown peer: r1_node_2
REGION PT2025/07/23 08:12:07 tree.go:210: r1_node_1 - eager push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940}] lazy push peers []
REGION PT2025/07/23 08:12:07 tree.go:217: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940}] lazy push peers []
REGION PT2025/07/23 08:12:07 tree.go:209: r1_node_1 - Processing onPeerDown peer: r1_node_2
REGION PT2025/07/23 08:12:07 tree.go:210: r1_node_1 - eager push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940}] lazy push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000}]
REGION PT2025/07/23 08:12:07 tree.go:217: r1_node_1 - eager push peers [{{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940}] lazy push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000}]
REGION PT2025/07/23 08:12:07 tree.go:209: r1_node_1 - Processing onPeerDown peer: r1_node_2
REGION PT2025/07/23 08:12:07 tree.go:210: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940}] lazy push peers [{{r1_node_2 r1_node_2:6001} 0xc0000a6700}]
REGION PT2025/07/23 08:12:07 tree.go:217: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940}] lazy push peers []
REGION HV2025/07/23 08:12:07 hyparview.go:288: r1_node_1 sent NEIGHBOR message to r1_node_6
GLOBAL HV2025/07/23 08:12:07 hyparview.go:288: r1_node_1 sent NEIGHBOR message to r1_node_5
REGION HV2025/07/23 08:12:07 hyparview.go:214: try lock
REGION HV2025/07/23 08:12:07 msg_handlers.go:256: try lock
REGION HV2025/07/23 08:12:07 msg_handlers.go:267: received NeighborReply self r1_node_1 from r1_node_6 accepted true attempts 1
REGION HV2025/07/23 08:12:07 msg_handlers.go:321: added peer to active view from neighbor reply peerID r1_node_6 address r1_node_6:6001
REGION PT2025/07/23 08:12:07 plumtree.go:325: r1_node_1 - Processing onPeerUp peer: r1_node_6
REGION PT2025/07/23 08:12:07 plumtree.go:326: try lock
REGION PT2025/07/23 08:12:07 plumtree.go:334: r1_node_1 - Added peer r1_node_6 to peers
REGION PT2025/07/23 08:12:07 tree.go:196: r1_node_1 - Processing onPeerUp peer: r1_node_6
REGION PT2025/07/23 08:12:07 tree.go:197: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000}] lazy push peers [{{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_8 r1_node_8:6001} 0xc00032b940}]
REGION PT2025/07/23 08:12:07 tree.go:202: r1_node_1 - Added peer r1_node_6 to eager push peers
REGION PT2025/07/23 08:12:07 tree.go:204: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_6 r1_node_6:6001} 0xc000242440}] lazy push peers [{{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_8 r1_node_8:6001} 0xc00032b940}]
REGION PT2025/07/23 08:12:07 tree.go:196: r1_node_1 - Processing onPeerUp peer: r1_node_6
REGION PT2025/07/23 08:12:07 tree.go:197: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940}] lazy push peers []
REGION PT2025/07/23 08:12:07 tree.go:202: r1_node_1 - Added peer r1_node_6 to eager push peers
REGION PT2025/07/23 08:12:07 tree.go:204: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940} {{r1_node_6 r1_node_6:6001} 0xc000242440}] lazy push peers []
REGION PT2025/07/23 08:12:07 tree.go:196: r1_node_1 - Processing onPeerUp peer: r1_node_6
REGION PT2025/07/23 08:12:07 tree.go:197: r1_node_1 - eager push peers [{{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940}] lazy push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000}]
REGION PT2025/07/23 08:12:07 tree.go:202: r1_node_1 - Added peer r1_node_6 to eager push peers
REGION PT2025/07/23 08:12:07 tree.go:204: r1_node_1 - eager push peers [{{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940} {{r1_node_6 r1_node_6:6001} 0xc000242440}] lazy push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000}]
REGION PT2025/07/23 08:12:07 tree.go:196: r1_node_1 - Processing onPeerUp peer: r1_node_6
REGION PT2025/07/23 08:12:07 tree.go:197: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940}] lazy push peers []
REGION PT2025/07/23 08:12:07 tree.go:202: r1_node_1 - Added peer r1_node_6 to eager push peers
REGION PT2025/07/23 08:12:07 tree.go:204: r1_node_1 - eager push peers [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940} {{r1_node_6 r1_node_6:6001} 0xc000242440}] lazy push peers []
GLOBAL HV2025/07/23 08:12:07 hyparview.go:214: try lock
GLOBAL HV2025/07/23 08:12:07 msg_handlers.go:256: try lock
GLOBAL HV2025/07/23 08:12:07 msg_handlers.go:267: received NeighborReply self r1_node_1 from r1_node_5 accepted true attempts 1
GLOBAL HV2025/07/23 08:12:07 msg_handlers.go:321: added peer to active view from neighbor reply peerID r1_node_5 address r1_node_5:7001
MONOCEROS2025/07/23 08:12:07 monoceros.go:775: try lock
MONOCEROS2025/07/23 08:12:07 monoceros.go:778: global network peer up
MONOCEROS2025/07/23 08:12:07 monoceros.go:780: already synced, no need to send sync req
2025/07/23 08:12:07 nothing to send to peer
MONOCEROS2025/07/23 08:12:07 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:07 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:07 monoceros.go:256: try promote
REGION PT2025/07/23 08:12:07 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:07 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:07 plumtree.go:220: r1_node_1 - [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940} {{r1_node_6 r1_node_6:6001} 0xc000242440}]
MONOCEROS2025/07/23 08:12:07 monoceros.go:266: peers num 5 now time 1753258327 expected aggregation time 1.7532583312e+09
MONOCEROS2025/07/23 08:12:07 monoceros.go:250: try lock
MONOCEROS2025/07/23 08:12:07 monoceros.go:256: try promote
MONOCEROS2025/07/23 08:12:07 monoceros.go:250: try lock
REGION PT2025/07/23 08:12:07 plumtree.go:216: try lock
REGION PT2025/07/23 08:12:07 plumtree.go:219: r1_node_1 - Get peers num
REGION PT2025/07/23 08:12:07 plumtree.go:220: r1_node_1 - [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940} {{r1_node_6 r1_node_6:6001} 0xc000242440}]
MONOCEROS2025/07/23 08:12:07 monoceros.go:266: peers num 5 now time 1753258327 expected aggregation time 1.7532583312e+09
MONOCEROS2025/07/23 08:12:07 monoceros.go:692: complete aggregation req &{RN 0xc000034200 1753258326 0 0xc000094bd0 <nil> 0 [0xc000301ec0] true 0x78c980 map[level:region regionID:r1]} {RN_r1_node_4 r1_node_4}
MONOCEROS2025/07/23 08:12:07 monoceros.go:693: <nil>
MONOCEROS2025/07/23 08:12:07 monoceros.go:694: RN_r1_node_4
REGION PT2025/07/23 08:12:07 plumtree.go:205: try lock
REGION PT2025/07/23 08:12:07 plumtree.go:208: r1_node_1 - Get parent [{{r1_node_3 r1_node_3:6001} 0xc00013e000} {{r1_node_5 r1_node_5:6001} 0xc0002423c0} {{r1_node_7 r1_node_7:6001} 0xc000298240} {{r1_node_8 r1_node_8:6001} 0xc00032b940} {{r1_node_6 r1_node_6:6001} 0xc000242440}]
MONOCEROS2025/07/23 08:12:07 monoceros.go:696: has parent
fatal error: sync: unlock of unlocked mutex

goroutine 151 [running]:
internal/sync.fatal({0x8ce5f5?, 0xc0000a6640?})
	/usr/local/go/src/runtime/panic.go:1068 +0x18
internal/sync.(*Mutex).unlockSlow(0xc000010500, 0xffffffff)
	/usr/local/go/src/internal/sync/mutex.go:204 +0x35
internal/sync.(*Mutex).Unlock(...)
	/usr/local/go/src/internal/sync/mutex.go:198
sync.(*Mutex).Unlock(...)
	/usr/local/go/src/sync/mutex.go:65
github.com/c12s/monoceros.(*Monoceros).completeAggregationReq(0xc0000fc280, 0xc0000fa5b0, {{0xc0003196e0, 0xc}, {0xc0003196f0, 0x9}}, 0x68809956, {0xc0003667d0, 0x2, 0x2}, ...)
	/app/monoceros.go:717 +0x8ef
github.com/c12s/monoceros.(*Monoceros).clearActive(0xc0000fc280, 0xc0000fa5b0, 0xc000301ec0, 0x0?)
	/app/monoceros.go:913 +0x18f
created by github.com/c12s/monoceros.(*Monoceros).processAggregationReq in goroutine 10
	/app/monoceros.go:686 +0x68e

goroutine 1 [chan receive]:
main.main()
	/app/cmd/main.go:170 +0x1047

goroutine 7 [select]:
github.com/c12s/hyparview/transport.Subscribe[...].func1()
	/hyparview/transport/subscription.go:15 +0x96
created by github.com/c12s/hyparview/transport.Subscribe[...] in goroutine 1
	/hyparview/transport/subscription.go:13 +0xab

goroutine 8 [chan receive]:
github.com/c12s/hyparview/hyparview.(*HyParView).OnPeerUp.func1()
	/hyparview/hyparview/hyparview.go:193 +0x88
created by github.com/c12s/hyparview/hyparview.(*HyParView).OnPeerUp in goroutine 1
	/hyparview/hyparview/hyparview.go:192 +0xa9

goroutine 9 [select]:
github.com/c12s/hyparview/transport.Subscribe[...].func1()
	/hyparview/transport/subscription.go:15 +0x96
created by github.com/c12s/hyparview/transport.Subscribe[...] in goroutine 1
	/hyparview/transport/subscription.go:13 +0xab

goroutine 10 [select]:
github.com/c12s/hyparview/transport.Subscribe[...].func1()
	/hyparview/transport/subscription.go:15 +0x165
created by github.com/c12s/hyparview/transport.Subscribe[...] in goroutine 1
	/hyparview/transport/subscription.go:13 +0xab

goroutine 11 [chan receive]:
github.com/c12s/hyparview/hyparview.(*HyParView).OnPeerUp.func1()
	/hyparview/hyparview/hyparview.go:193 +0x88
created by github.com/c12s/hyparview/hyparview.(*HyParView).OnPeerUp in goroutine 1
	/hyparview/hyparview/hyparview.go:192 +0xa9

goroutine 12 [chan receive]:
github.com/c12s/hyparview/hyparview.(*HyParView).OnPeerDown.func1()
	/hyparview/hyparview/hyparview.go:203 +0x88
created by github.com/c12s/hyparview/hyparview.(*HyParView).OnPeerDown in goroutine 1
	/hyparview/hyparview/hyparview.go:202 +0xa9

goroutine 13 [select]:
github.com/c12s/hyparview/transport.Subscribe[...].func1()
	/hyparview/transport/subscription.go:15 +0x96
created by github.com/c12s/hyparview/transport.Subscribe[...] in goroutine 1
	/hyparview/transport/subscription.go:13 +0xab

goroutine 14 [select]:
github.com/c12s/hyparview/transport.Subscribe[...].func1()
	/hyparview/transport/subscription.go:15 +0x165
created by github.com/c12s/hyparview/transport.Subscribe[...] in goroutine 1
	/hyparview/transport/subscription.go:13 +0xab

goroutine 15 [chan receive]:
github.com/c12s/hyparview/hyparview.(*HyParView).OnPeerUp.func1()
	/hyparview/hyparview/hyparview.go:193 +0x88
created by github.com/c12s/hyparview/hyparview.(*HyParView).OnPeerUp in goroutine 1
	/hyparview/hyparview/hyparview.go:192 +0xa9

goroutine 16 [chan receive]:
github.com/c12s/hyparview/hyparview.(*HyParView).OnPeerDown.func1()
	/hyparview/hyparview/hyparview.go:203 +0x88
created by github.com/c12s/hyparview/hyparview.(*HyParView).OnPeerDown in goroutine 1
	/hyparview/hyparview/hyparview.go:202 +0xa9

goroutine 17 [IO wait]:
internal/poll.runtime_pollWait(0x7f33b82c3670, 0x72)
	/usr/local/go/src/runtime/netpoll.go:351 +0x85
internal/poll.(*pollDesc).wait(0xc000034300?, 0x4b871e?, 0x0)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:84 +0x27
internal/poll.(*pollDesc).waitRead(...)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Accept(0xc000034300)
	/usr/local/go/src/internal/poll/fd_unix.go:620 +0x295
net.(*netFD).accept(0xc000034300)
	/usr/local/go/src/net/fd_unix.go:172 +0x29
net.(*TCPListener).accept(0xc0000a65c0)
	/usr/local/go/src/net/tcpsock_posix.go:159 +0x1b
net.(*TCPListener).Accept(0xc0000a65c0)
	/usr/local/go/src/net/tcpsock.go:380 +0x30
main.main.AcceptTcpConnsFn.func3.1({0x96b070, 0xc0000a65c0})
	/hyparview/transport/conn_tcp.go:154 +0x63
created by main.main.AcceptTcpConnsFn.func3 in goroutine 1
	/hyparview/transport/conn_tcp.go:152 +0x185

goroutine 18 [chan receive]:
main.main.AcceptTcpConnsFn.func3.2(0x0?, {0x96b070, 0xc0000a65c0})
	/hyparview/transport/conn_tcp.go:170 +0x35
created by main.main.AcceptTcpConnsFn.func3 in goroutine 1
	/hyparview/transport/conn_tcp.go:169 +0x248

goroutine 19 [select]:
github.com/c12s/hyparview/transport.Subscribe[...].func1()
	/hyparview/transport/subscription.go:15 +0xf2
created by github.com/c12s/hyparview/transport.Subscribe[...] in goroutine 1
	/hyparview/transport/subscription.go:13 +0xab

goroutine 20 [select]:
github.com/c12s/hyparview/hyparview.(*HyParView).shuffle(0xc0000000e0)
	/hyparview/hyparview/hyparview.go:336 +0x96
created by github.com/c12s/hyparview/hyparview.(*HyParView).Join in goroutine 1
	/hyparview/hyparview/hyparview.go:90 +0x427

goroutine 21 [IO wait]:
internal/poll.runtime_pollWait(0x7f33b82c3558, 0x72)
	/usr/local/go/src/runtime/netpoll.go:351 +0x85
internal/poll.(*pollDesc).wait(0xc000034380?, 0x4b871e?, 0x0)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:84 +0x27
internal/poll.(*pollDesc).waitRead(...)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Accept(0xc000034380)
	/usr/local/go/src/internal/poll/fd_unix.go:620 +0x295
net.(*netFD).accept(0xc000034380)
	/usr/local/go/src/net/fd_unix.go:172 +0x29
net.(*TCPListener).accept(0xc0000a6600)
	/usr/local/go/src/net/tcpsock_posix.go:159 +0x1b
net.(*TCPListener).Accept(0xc0000a6600)
	/usr/local/go/src/net/tcpsock.go:380 +0x30
main.main.AcceptTcpConnsFn.func4.1({0x96b070, 0xc0000a6600})
	/hyparview/transport/conn_tcp.go:154 +0x63
created by main.main.AcceptTcpConnsFn.func4 in goroutine 1
	/hyparview/transport/conn_tcp.go:152 +0x185

goroutine 22 [chan receive]:
main.main.AcceptTcpConnsFn.func4.2(0x0?, {0x96b070, 0xc0000a6600})
	/hyparview/transport/conn_tcp.go:170 +0x35
created by main.main.AcceptTcpConnsFn.func4 in goroutine 1
	/hyparview/transport/conn_tcp.go:169 +0x248

goroutine 23 [select]:
github.com/c12s/hyparview/transport.Subscribe[...].func1()
	/hyparview/transport/subscription.go:15 +0xf2
created by github.com/c12s/hyparview/transport.Subscribe[...] in goroutine 1
	/hyparview/transport/subscription.go:13 +0xab

goroutine 24 [select]:
github.com/c12s/hyparview/hyparview.(*HyParView).shuffle(0xc0000001c0)
	/hyparview/hyparview/hyparview.go:336 +0x96
created by github.com/c12s/hyparview/hyparview.(*HyParView).Join in goroutine 1
	/hyparview/hyparview/hyparview.go:90 +0x427

goroutine 25 [chan receive]:
github.com/c12s/monoceros.(*Monoceros).tryPromote(0xc0000fc280, 0xc0000fa5b0)
	/app/monoceros.go:249 +0x68
created by github.com/c12s/monoceros.(*Monoceros).initRN in goroutine 1
	/app/monoceros.go:201 +0x21a

goroutine 26 [chan receive]:
github.com/c12s/monoceros.(*Monoceros).initAggregation(0xc0000fc280, 0xc0000fa5b0)
	/app/monoceros.go:304 +0x50
created by github.com/c12s/monoceros.(*Monoceros).init in goroutine 1
	/app/monoceros.go:184 +0x78

goroutine 27 [chan receive]:
github.com/c12s/monoceros.(*Monoceros).tryTriggerAggregation(...)
	/app/monoceros.go:295
created by github.com/c12s/monoceros.(*Monoceros).init in goroutine 1
	/app/monoceros.go:185 +0xd4

goroutine 28 [chan receive]:
github.com/c12s/monoceros.(*Monoceros).tryPromote(0xc0000fc280, 0xc0000fa620)
	/app/monoceros.go:249 +0x68
created by github.com/c12s/monoceros.(*Monoceros).initRRN in goroutine 1
	/app/monoceros.go:211 +0x1b1

goroutine 29 [chan receive]:
github.com/c12s/monoceros.(*Monoceros).initAggregation(0xc0000fc280, 0xc0000fa620)
	/app/monoceros.go:304 +0x50
created by github.com/c12s/monoceros.(*Monoceros).init in goroutine 1
	/app/monoceros.go:188 +0x13a

goroutine 30 [chan receive]:
github.com/c12s/monoceros.(*Monoceros).tryTriggerAggregation(...)
	/app/monoceros.go:295
created by github.com/c12s/monoceros.(*Monoceros).init in goroutine 1
	/app/monoceros.go:189 +0x196

goroutine 31 [chan receive]:
github.com/c12s/monoceros.(*Monoceros).Start.func1()
	/app/monoceros.go:176 +0x66
created by github.com/c12s/monoceros.(*Monoceros).Start in goroutine 1
	/app/monoceros.go:175 +0x216

goroutine 32 [IO wait]:
internal/poll.runtime_pollWait(0x7f33b82c3440, 0x72)
	/usr/local/go/src/runtime/netpoll.go:351 +0x85
internal/poll.(*pollDesc).wait(0xc00029a080?, 0x380016?, 0x0)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:84 +0x27
internal/poll.(*pollDesc).waitRead(...)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Accept(0xc00029a080)
	/usr/local/go/src/internal/poll/fd_unix.go:620 +0x295
net.(*netFD).accept(0xc00029a080)
	/usr/local/go/src/net/fd_unix.go:172 +0x29
net.(*TCPListener).accept(0xc000298040)
	/usr/local/go/src/net/tcpsock_posix.go:159 +0x1b
net.(*TCPListener).Accept(0xc000298040)
	/usr/local/go/src/net/tcpsock.go:380 +0x30
net/http.(*Server).Serve(0xc00019e100, {0x96b070, 0xc000298040})
	/usr/local/go/src/net/http/server.go:3424 +0x30c
net/http.(*Server).ListenAndServe(0xc00019e100)
	/usr/local/go/src/net/http/server.go:3350 +0x71
main.main.func1()
	/app/cmd/main.go:161 +0x17
created by main.main in goroutine 1
	/app/cmd/main.go:160 +0xf93

goroutine 49 [syscall]:
os/signal.signal_recv()
	/usr/local/go/src/runtime/sigqueue.go:152 +0x29
os/signal.loop()
	/usr/local/go/src/os/signal/signal_unix.go:23 +0x13
created by os/signal.Notify.func1.1 in goroutine 1
	/usr/local/go/src/os/signal/signal.go:152 +0x1f

goroutine 110 [chan receive]:
github.com/c12s/hyparview/transport.(*TCPConn).onReceive.func1()
	/hyparview/transport/conn_tcp.go:97 +0x6c
created by github.com/c12s/hyparview/transport.(*TCPConn).onReceive in goroutine 9
	/hyparview/transport/conn_tcp.go:96 +0x67

goroutine 50 [chan receive]:
github.com/c12s/hyparview/transport.MakeTCPConn.func1()
	/hyparview/transport/conn_tcp.go:48 +0x31
created by github.com/c12s/hyparview/transport.MakeTCPConn in goroutine 17
	/hyparview/transport/conn_tcp.go:47 +0x147

goroutine 140 [IO wait]:
internal/poll.runtime_pollWait(0x7f33b8281c90, 0x72)
	/usr/local/go/src/runtime/netpoll.go:351 +0x85
internal/poll.(*pollDesc).wait(0xc000034a80?, 0xc000204f2c?, 0x0)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:84 +0x27
internal/poll.(*pollDesc).waitRead(...)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc000034a80, {0xc000204f2c, 0x4, 0x4})
	/usr/local/go/src/internal/poll/fd_unix.go:165 +0x27a
net.(*netFD).Read(0xc000034a80, {0xc000204f2c?, 0x40c877?, 0xc000095b90?})
	/usr/local/go/src/net/fd_posix.go:55 +0x25
net.(*conn).Read(0xc0000682b8, {0xc000204f2c?, 0x0?, 0x0?})
	/usr/local/go/src/net/net.go:194 +0x45
github.com/c12s/hyparview/transport.(*TCPConn).read.func1()
	/hyparview/transport/conn_tcp.go:107 +0x85
created by github.com/c12s/hyparview/transport.(*TCPConn).read in goroutine 7
	/hyparview/transport/conn_tcp.go:104 +0x4f

goroutine 56 [chan receive]:
github.com/c12s/hyparview/transport.MakeTCPConn.func1()
	/hyparview/transport/conn_tcp.go:48 +0x31
created by github.com/c12s/hyparview/transport.MakeTCPConn in goroutine 17
	/hyparview/transport/conn_tcp.go:47 +0x147

goroutine 53 [chan receive]:
github.com/c12s/hyparview/transport.(*TCPConn).onReceive.func1()
	/hyparview/transport/conn_tcp.go:97 +0x6c
created by github.com/c12s/hyparview/transport.(*TCPConn).onReceive in goroutine 52
	/hyparview/transport/conn_tcp.go:96 +0x67

goroutine 34 [chan receive]:
github.com/c12s/hyparview/transport.MakeTCPConn.func1()
	/hyparview/transport/conn_tcp.go:48 +0x31
created by github.com/c12s/hyparview/transport.MakeTCPConn in goroutine 21
	/hyparview/transport/conn_tcp.go:47 +0x147

goroutine 109 [IO wait]:
internal/poll.runtime_pollWait(0x7f33b8281ec0, 0x72)
	/usr/local/go/src/runtime/netpoll.go:351 +0x85
internal/poll.(*pollDesc).wait(0xc00022a480?, 0xc000141f2c?, 0x0)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:84 +0x27
internal/poll.(*pollDesc).waitRead(...)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc00022a480, {0xc000141f2c, 0x4, 0x4})
	/usr/local/go/src/internal/poll/fd_unix.go:165 +0x27a
net.(*netFD).Read(0xc00022a480, {0xc000141f2c?, 0x40c877?, 0xc000240700?})
	/usr/local/go/src/net/fd_posix.go:55 +0x25
net.(*conn).Read(0xc000236080, {0xc000141f2c?, 0x8d0c47?, 0x22?})
	/usr/local/go/src/net/net.go:194 +0x45
github.com/c12s/hyparview/transport.(*TCPConn).read.func1()
	/hyparview/transport/conn_tcp.go:107 +0x85
created by github.com/c12s/hyparview/transport.(*TCPConn).read in goroutine 9
	/hyparview/transport/conn_tcp.go:104 +0x4f

goroutine 99 [chan receive]:
github.com/c12s/hyparview/transport.MakeTCPConn.func1()
	/hyparview/transport/conn_tcp.go:48 +0x31
created by github.com/c12s/hyparview/transport.MakeTCPConn in goroutine 23
	/hyparview/transport/conn_tcp.go:47 +0x147

goroutine 37 [chan receive]:
github.com/c12s/hyparview/transport.(*TCPConn).onReceive.func1()
	/hyparview/transport/conn_tcp.go:97 +0x6c
created by github.com/c12s/hyparview/transport.(*TCPConn).onReceive in goroutine 36
	/hyparview/transport/conn_tcp.go:96 +0x67

goroutine 57 [IO wait]:
internal/poll.runtime_pollWait(0x7f33b82c30f8, 0x72)
	/usr/local/go/src/runtime/netpoll.go:351 +0x85
internal/poll.(*pollDesc).wait(0xc00022a100?, 0xc0001a4f2c?, 0x0)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:84 +0x27
internal/poll.(*pollDesc).waitRead(...)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc00022a100, {0xc0001a4f2c, 0x4, 0x4})
	/usr/local/go/src/internal/poll/fd_unix.go:165 +0x27a
net.(*netFD).Read(0xc00022a100, {0xc0001a4f2c?, 0x40c877?, 0xc0002400e0?})
	/usr/local/go/src/net/fd_posix.go:55 +0x25
net.(*conn).Read(0xc000236020, {0xc0001a4f2c?, 0x38?, 0x38?})
	/usr/local/go/src/net/net.go:194 +0x45
github.com/c12s/hyparview/transport.(*TCPConn).read.func1()
	/hyparview/transport/conn_tcp.go:107 +0x85
created by github.com/c12s/hyparview/transport.(*TCPConn).read in goroutine 17
	/hyparview/transport/conn_tcp.go:104 +0x4f

goroutine 81 [chan receive]:
github.com/c12s/hyparview/transport.MakeTCPConn.func1()
	/hyparview/transport/conn_tcp.go:48 +0x31
created by github.com/c12s/hyparview/transport.MakeTCPConn in goroutine 23
	/hyparview/transport/conn_tcp.go:47 +0x147

goroutine 59 [chan receive]:
github.com/c12s/hyparview/transport.(*TCPConn).onReceive.func1()
	/hyparview/transport/conn_tcp.go:97 +0x6c
created by github.com/c12s/hyparview/transport.(*TCPConn).onReceive in goroutine 58
	/hyparview/transport/conn_tcp.go:96 +0x67

goroutine 83 [chan receive]:
github.com/c12s/hyparview/transport.(*TCPConn).onReceive.func1()
	/hyparview/transport/conn_tcp.go:97 +0x6c
created by github.com/c12s/hyparview/transport.(*TCPConn).onReceive in goroutine 23
	/hyparview/transport/conn_tcp.go:96 +0x67

goroutine 82 [IO wait]:
internal/poll.runtime_pollWait(0x7f33b82c2fe0, 0x72)
	/usr/local/go/src/runtime/netpoll.go:351 +0x85
internal/poll.(*pollDesc).wait(0xc000132080?, 0xc00014472c?, 0x0)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:84 +0x27
internal/poll.(*pollDesc).waitRead(...)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc000132080, {0xc00014472c, 0x4, 0x4})
	/usr/local/go/src/internal/poll/fd_unix.go:165 +0x27a
net.(*netFD).Read(0xc000132080, {0xc00014472c?, 0x40c877?, 0xc00013c000?})
	/usr/local/go/src/net/fd_posix.go:55 +0x25
net.(*conn).Read(0xc000138000, {0xc00014472c?, 0x0?, 0x0?})
	/usr/local/go/src/net/net.go:194 +0x45
github.com/c12s/hyparview/transport.(*TCPConn).read.func1()
	/hyparview/transport/conn_tcp.go:107 +0x85
created by github.com/c12s/hyparview/transport.(*TCPConn).read in goroutine 23
	/hyparview/transport/conn_tcp.go:104 +0x4f

goroutine 63 [chan receive]:
github.com/c12s/hyparview/transport.MakeTCPConn.func1()
	/hyparview/transport/conn_tcp.go:48 +0x31
created by github.com/c12s/hyparview/transport.MakeTCPConn in goroutine 19
	/hyparview/transport/conn_tcp.go:47 +0x147

goroutine 97 [chan receive]:
github.com/c12s/hyparview/transport.(*TCPConn).onReceive.func1()
	/hyparview/transport/conn_tcp.go:97 +0x6c
created by github.com/c12s/hyparview/transport.(*TCPConn).onReceive in goroutine 19
	/hyparview/transport/conn_tcp.go:96 +0x67

goroutine 64 [IO wait]:
internal/poll.runtime_pollWait(0x7f33b82c2ec8, 0x72)
	/usr/local/go/src/runtime/netpoll.go:351 +0x85
internal/poll.(*pollDesc).wait(0xc00022a280?, 0xc00020af2c?, 0x0)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:84 +0x27
internal/poll.(*pollDesc).waitRead(...)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc00022a280, {0xc00020af2c, 0x4, 0x4})
	/usr/local/go/src/internal/poll/fd_unix.go:165 +0x27a
net.(*netFD).Read(0xc00022a280, {0xc00020af2c?, 0x40c877?, 0xc0002403f0?})
	/usr/local/go/src/net/fd_posix.go:55 +0x25
net.(*conn).Read(0xc000236060, {0xc00020af2c?, 0x0?, 0x0?})
	/usr/local/go/src/net/net.go:194 +0x45
github.com/c12s/hyparview/transport.(*TCPConn).read.func1()
	/hyparview/transport/conn_tcp.go:107 +0x85
created by github.com/c12s/hyparview/transport.(*TCPConn).read in goroutine 19
	/hyparview/transport/conn_tcp.go:104 +0x4f

goroutine 103 [chan receive]:
github.com/c12s/hyparview/transport.MakeTCPConn.func1()
	/hyparview/transport/conn_tcp.go:48 +0x31
created by github.com/c12s/hyparview/transport.MakeTCPConn in goroutine 21
	/hyparview/transport/conn_tcp.go:47 +0x147

goroutine 149 [chan receive]:
github.com/c12s/hyparview/transport.(*TCPConn).onReceive.func1()
	/hyparview/transport/conn_tcp.go:97 +0x6c
created by github.com/c12s/hyparview/transport.(*TCPConn).onReceive in goroutine 9
	/hyparview/transport/conn_tcp.go:96 +0x67

goroutine 101 [chan receive]:
github.com/c12s/hyparview/transport.(*TCPConn).onReceive.func1()
	/hyparview/transport/conn_tcp.go:97 +0x6c
created by github.com/c12s/hyparview/transport.(*TCPConn).onReceive in goroutine 23
	/hyparview/transport/conn_tcp.go:96 +0x67

goroutine 104 [IO wait]:
internal/poll.runtime_pollWait(0x7f33b82c2db0, 0x72)
	/usr/local/go/src/runtime/netpoll.go:351 +0x85
internal/poll.(*pollDesc).wait(0xc00022a400?, 0xc00014172c?, 0x0)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:84 +0x27
internal/poll.(*pollDesc).waitRead(...)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc00022a400, {0xc00014172c, 0x4, 0x4})
	/usr/local/go/src/internal/poll/fd_unix.go:165 +0x27a
net.(*netFD).Read(0xc00022a400, {0xc00014172c?, 0x40c877?, 0xc000240620?})
	/usr/local/go/src/net/fd_posix.go:55 +0x25
net.(*conn).Read(0xc000236078, {0xc00014172c?, 0x0?, 0x0?})
	/usr/local/go/src/net/net.go:194 +0x45
github.com/c12s/hyparview/transport.(*TCPConn).read.func1()
	/hyparview/transport/conn_tcp.go:107 +0x85
created by github.com/c12s/hyparview/transport.(*TCPConn).read in goroutine 21
	/hyparview/transport/conn_tcp.go:104 +0x4f

goroutine 106 [chan receive]:
github.com/c12s/hyparview/transport.(*TCPConn).onReceive.func1()
	/hyparview/transport/conn_tcp.go:97 +0x6c
created by github.com/c12s/hyparview/transport.(*TCPConn).onReceive in goroutine 105
	/hyparview/transport/conn_tcp.go:96 +0x67

goroutine 118 [chan receive]:
github.com/c12s/hyparview/transport.MakeTCPConn.func1()
	/hyparview/transport/conn_tcp.go:48 +0x31
created by github.com/c12s/hyparview/transport.MakeTCPConn in goroutine 19
	/hyparview/transport/conn_tcp.go:47 +0x147

goroutine 45 [IO wait]:
internal/poll.runtime_pollWait(0x7f33b82c2b80, 0x72)
	/usr/local/go/src/runtime/netpoll.go:351 +0x85
internal/poll.(*pollDesc).wait(0xc000034680?, 0xc0001f8000?, 0x0)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:84 +0x27
internal/poll.(*pollDesc).waitRead(...)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc000034680, {0xc0001f8000, 0x1000, 0x1000})
	/usr/local/go/src/internal/poll/fd_unix.go:165 +0x27a
net.(*netFD).Read(0xc000034680, {0xc0001f8000?, 0x404ef4?, 0x0?})
	/usr/local/go/src/net/fd_posix.go:55 +0x25
net.(*conn).Read(0xc000068270, {0xc0001f8000?, 0x404cef?, 0xc0001cb470?})
	/usr/local/go/src/net/net.go:194 +0x45
net/http.(*persistConn).Read(0xc0000db320, {0xc0001f8000?, 0x64d365?, 0x967c00?})
	/usr/local/go/src/net/http/transport.go:2122 +0x47
bufio.(*Reader).fill(0xc000088a20)
	/usr/local/go/src/bufio/bufio.go:113 +0x103
bufio.(*Reader).Peek(0xc000088a20, 0x1)
	/usr/local/go/src/bufio/bufio.go:152 +0x53
net/http.(*persistConn).readLoop(0xc0000db320)
	/usr/local/go/src/net/http/transport.go:2275 +0x172
created by net/http.(*Transport).dialConn in goroutine 43
	/usr/local/go/src/net/http/transport.go:1944 +0x174f

goroutine 46 [select]:
net/http.(*persistConn).writeLoop(0xc0000db320)
	/usr/local/go/src/net/http/transport.go:2590 +0xe7
created by net/http.(*Transport).dialConn in goroutine 43
	/usr/local/go/src/net/http/transport.go:1945 +0x17a5

goroutine 66 [sleep]:
time.Sleep(0x12a05f200)
	/usr/local/go/src/runtime/time.go:338 +0x165
github.com/c12s/monoceros.(*Monoceros).clearActive(0xc0000fc280, 0xc0000fa5b0, 0xc000300ae0, 0xc0000000e0?)
	/app/monoceros.go:907 +0x8b
created by github.com/c12s/monoceros.(*Monoceros).processAggregationReq in goroutine 26
	/app/monoceros.go:686 +0x68e

goroutine 119 [IO wait]:
internal/poll.runtime_pollWait(0x7f33b82c2950, 0x72)
	/usr/local/go/src/runtime/netpoll.go:351 +0x85
internal/poll.(*pollDesc).wait(0xc000328600?, 0xc000408f2c?, 0x0)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:84 +0x27
internal/poll.(*pollDesc).waitRead(...)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc000328600, {0xc000408f2c, 0x4, 0x4})
	/usr/local/go/src/internal/poll/fd_unix.go:165 +0x27a
net.(*netFD).Read(0xc000328600, {0xc000408f2c?, 0x40c877?, 0xc0002ca4d0?})
	/usr/local/go/src/net/fd_posix.go:55 +0x25
net.(*conn).Read(0xc0002a2140, {0xc000408f2c?, 0x0?, 0x0?})
	/usr/local/go/src/net/net.go:194 +0x45
github.com/c12s/hyparview/transport.(*TCPConn).read.func1()
	/hyparview/transport/conn_tcp.go:107 +0x85
created by github.com/c12s/hyparview/transport.(*TCPConn).read in goroutine 19
	/hyparview/transport/conn_tcp.go:104 +0x4f

goroutine 147 [chan receive]:
github.com/c12s/hyparview/transport.MakeTCPConn.func1()
	/hyparview/transport/conn_tcp.go:48 +0x31
created by github.com/c12s/hyparview/transport.MakeTCPConn in goroutine 9
	/hyparview/transport/conn_tcp.go:47 +0x147

goroutine 120 [chan receive]:
github.com/c12s/hyparview/transport.(*TCPConn).onReceive.func1()
	/hyparview/transport/conn_tcp.go:97 +0x6c
created by github.com/c12s/hyparview/transport.(*TCPConn).onReceive in goroutine 19
	/hyparview/transport/conn_tcp.go:96 +0x67

goroutine 114 [chan receive]:
github.com/c12s/hyparview/transport.MakeTCPConn.func1()
	/hyparview/transport/conn_tcp.go:48 +0x31
created by github.com/c12s/hyparview/transport.MakeTCPConn in goroutine 23
	/hyparview/transport/conn_tcp.go:47 +0x147

goroutine 116 [chan receive]:
github.com/c12s/hyparview/transport.(*TCPConn).onReceive.func1()
	/hyparview/transport/conn_tcp.go:97 +0x6c
created by github.com/c12s/hyparview/transport.(*TCPConn).onReceive in goroutine 23
	/hyparview/transport/conn_tcp.go:96 +0x67

goroutine 115 [IO wait]:
internal/poll.runtime_pollWait(0x7f33b82c2a68, 0x72)
	/usr/local/go/src/runtime/netpoll.go:351 +0x85
internal/poll.(*pollDesc).wait(0xc00029a200?, 0xc0001a372c?, 0x0)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:84 +0x27
internal/poll.(*pollDesc).waitRead(...)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc00029a200, {0xc0001a372c, 0x4, 0x4})
	/usr/local/go/src/internal/poll/fd_unix.go:165 +0x27a
net.(*netFD).Read(0xc00029a200, {0xc0001a372c?, 0x40c877?, 0xc0002ca2a0?})
	/usr/local/go/src/net/fd_posix.go:55 +0x25
net.(*conn).Read(0xc0002a2020, {0xc0001a372c?, 0x0?, 0x0?})
	/usr/local/go/src/net/net.go:194 +0x45
github.com/c12s/hyparview/transport.(*TCPConn).read.func1()
	/hyparview/transport/conn_tcp.go:107 +0x85
created by github.com/c12s/hyparview/transport.(*TCPConn).read in goroutine 23
	/hyparview/transport/conn_tcp.go:104 +0x4f

goroutine 148 [IO wait]:
internal/poll.runtime_pollWait(0x7f33b82c2838, 0x72)
	/usr/local/go/src/runtime/netpoll.go:351 +0x85
internal/poll.(*pollDesc).wait(0xc000328700?, 0xc00006072c?, 0x0)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:84 +0x27
internal/poll.(*pollDesc).waitRead(...)
	/usr/local/go/src/internal/poll/fd_poll_runtime.go:89
internal/poll.(*FD).Read(0xc000328700, {0xc00006072c, 0x4, 0x4})
	/usr/local/go/src/internal/poll/fd_unix.go:165 +0x27a
net.(*netFD).Read(0xc000328700, {0xc00006072c?, 0x40c877?, 0xc000340700?})
	/usr/local/go/src/net/fd_posix.go:55 +0x25
net.(*conn).Read(0xc00032e290, {0xc00006072c?, 0x0?, 0x0?})
	/usr/local/go/src/net/net.go:194 +0x45
github.com/c12s/hyparview/transport.(*TCPConn).read.func1()
	/hyparview/transport/conn_tcp.go:107 +0x85
created by github.com/c12s/hyparview/transport.(*TCPConn).read in goroutine 9
	/hyparview/transport/conn_tcp.go:104 +0x4f

goroutine 139 [chan receive]:
github.com/c12s/hyparview/transport.MakeTCPConn.func1()
	/hyparview/transport/conn_tcp.go:48 +0x31
created by github.com/c12s/hyparview/transport.MakeTCPConn in goroutine 7
	/hyparview/transport/conn_tcp.go:47 +0x147

goroutine 108 [chan receive]:
github.com/c12s/hyparview/transport.MakeTCPConn.func1()
	/hyparview/transport/conn_tcp.go:48 +0x31
created by github.com/c12s/hyparview/transport.MakeTCPConn in goroutine 9
	/hyparview/transport/conn_tcp.go:47 +0x147

goroutine 141 [chan receive]:
github.com/c12s/hyparview/transport.(*TCPConn).onReceive.func1()
	/hyparview/transport/conn_tcp.go:97 +0x6c
created by github.com/c12s/hyparview/transport.(*TCPConn).onReceive in goroutine 7
	/hyparview/transport/conn_tcp.go:96 +0x67
